{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My First Project\n",
    "This is the first project in which I will practice the skills I have been learning along my journey to becoming a data scientist.\n",
    "\n",
    "In this case, we are going to analyze data from Lending Club. This platform connects borrowers with investors, and manages all financial trades. It operates as a peer to peer lending system regulated by the plataform itself.\n",
    "\n",
    "The dataset was download from Kaggle - Lending Club Loan Data\n",
    "\n",
    "Let´s load the provided information as a csv with the library Pandas.\n",
    "\n",
    "(In SQL we will query, filter, group and transform data efficiently.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kz/rybrs64563zc2gl94rf00qbw0000gn/T/ipykernel_4885/4253356958.py:6: DtypeWarning: Columns (19,47,55,112,123,124,125,128,129,130,133,139,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(filepath)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load dataset\n",
    "filepath = '/Users/hectorastudillo/Downloads/archive/Lending_data.csv'\n",
    "\n",
    "data = pd.read_csv(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploring\n",
    "Then, we need to understand our data by summarizing how many columns are in the file, identifying the data type of each one, we also want to analyze the information further and think: How can I use all this data?\n",
    "\n",
    "- Obtain simple information of the columns, rows, a especific number of entries:\n",
    "    - .info()\n",
    "    - .head()\n",
    "    - .columns\n",
    "    - num_rows, num_columns = data.shape - This return me 2 variables.\n",
    "\n",
    "- Obtain an specific column and row \n",
    "    - print(data.['Column'][1]) - I can save this into a variable too.\n",
    "    - ILOC \n",
    "        - .iloc[] - This returns the rows specified to you, using the same indexing as arrays.\n",
    "        - .iloc[] - To create a specific subset\n",
    "        - .iloc[x,y] - For accesing cells\n",
    "\n",
    "- Obtain the type and dataset:\n",
    "    -type(dataset)\n",
    "    - print(dataset)\n",
    "\n",
    "- Obtain statitic information: \n",
    "    - describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2260668 entries, 0 to 2260667\n",
      "Columns: 145 entries, id to settlement_term\n",
      "dtypes: float64(105), int64(4), object(36)\n",
      "memory usage: 2.4+ GB\n",
      "Data obtained with info()\n",
      " None\n",
      "Data obtained with head()\n",
      "    id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
      "0 NaN        NaN       2500         2500           2500.0   36 months   \n",
      "1 NaN        NaN      30000        30000          30000.0   60 months   \n",
      "2 NaN        NaN       5000         5000           5000.0   36 months   \n",
      "3 NaN        NaN       4000         4000           4000.0   36 months   \n",
      "4 NaN        NaN      30000        30000          30000.0   60 months   \n",
      "\n",
      "   int_rate  installment grade sub_grade  ... hardship_payoff_balance_amount  \\\n",
      "0     13.56        84.92     C        C1  ...                            NaN   \n",
      "1     18.94       777.23     D        D2  ...                            NaN   \n",
      "2     17.97       180.69     D        D1  ...                            NaN   \n",
      "3     18.94       146.51     D        D2  ...                            NaN   \n",
      "4     16.14       731.78     C        C4  ...                            NaN   \n",
      "\n",
      "  hardship_last_payment_amount disbursement_method  debt_settlement_flag  \\\n",
      "0                          NaN                Cash                     N   \n",
      "1                          NaN                Cash                     N   \n",
      "2                          NaN                Cash                     N   \n",
      "3                          NaN                Cash                     N   \n",
      "4                          NaN                Cash                     N   \n",
      "\n",
      "  debt_settlement_flag_date settlement_status settlement_date  \\\n",
      "0                       NaN               NaN             NaN   \n",
      "1                       NaN               NaN             NaN   \n",
      "2                       NaN               NaN             NaN   \n",
      "3                       NaN               NaN             NaN   \n",
      "4                       NaN               NaN             NaN   \n",
      "\n",
      "  settlement_amount  settlement_percentage settlement_term  \n",
      "0               NaN                    NaN             NaN  \n",
      "1               NaN                    NaN             NaN  \n",
      "2               NaN                    NaN             NaN  \n",
      "3               NaN                    NaN             NaN  \n",
      "4               NaN                    NaN             NaN  \n",
      "\n",
      "[5 rows x 145 columns]\n",
      "Data obtained with describe()\n",
      "         id  member_id     loan_amnt   funded_amnt  funded_amnt_inv  \\\n",
      "count  0.0        0.0  2.260668e+06  2.260668e+06     2.260668e+06   \n",
      "mean   NaN        NaN  1.504693e+04  1.504166e+04     1.502344e+04   \n",
      "std    NaN        NaN  9.190245e+03  9.188413e+03     9.192332e+03   \n",
      "min    NaN        NaN  5.000000e+02  5.000000e+02     0.000000e+00   \n",
      "25%    NaN        NaN  8.000000e+03  8.000000e+03     8.000000e+03   \n",
      "50%    NaN        NaN  1.290000e+04  1.287500e+04     1.280000e+04   \n",
      "75%    NaN        NaN  2.000000e+04  2.000000e+04     2.000000e+04   \n",
      "max    NaN        NaN  4.000000e+04  4.000000e+04     4.000000e+04   \n",
      "\n",
      "           int_rate   installment    annual_inc  url           dti  ...  \\\n",
      "count  2.260668e+06  2.260668e+06  2.260664e+06  0.0  2.258957e+06  ...   \n",
      "mean   1.309291e+01  4.458076e+02  7.799243e+04  NaN  1.882420e+01  ...   \n",
      "std    4.832114e+00  2.671737e+02  1.126962e+05  NaN  1.418333e+01  ...   \n",
      "min    5.310000e+00  4.930000e+00  0.000000e+00  NaN -1.000000e+00  ...   \n",
      "25%    9.490000e+00  2.516500e+02  4.600000e+04  NaN  1.189000e+01  ...   \n",
      "50%    1.262000e+01  3.779900e+02  6.500000e+04  NaN  1.784000e+01  ...   \n",
      "75%    1.599000e+01  5.933200e+02  9.300000e+04  NaN  2.449000e+01  ...   \n",
      "max    3.099000e+01  1.719830e+03  1.100000e+08  NaN  9.990000e+02  ...   \n",
      "\n",
      "       deferral_term  hardship_amount  hardship_length  hardship_dpd  \\\n",
      "count        10613.0     10613.000000          10613.0  10613.000000   \n",
      "mean             3.0       155.006696              3.0     13.686422   \n",
      "std              0.0       129.113137              0.0      9.728138   \n",
      "min              3.0         0.640000              3.0      0.000000   \n",
      "25%              3.0        59.370000              3.0      5.000000   \n",
      "50%              3.0       119.040000              3.0     15.000000   \n",
      "75%              3.0       213.260000              3.0     22.000000   \n",
      "max              3.0       943.940000              3.0     37.000000   \n",
      "\n",
      "       orig_projected_additional_accrued_interest  \\\n",
      "count                                 8426.000000   \n",
      "mean                                   454.840802   \n",
      "std                                    375.830737   \n",
      "min                                      1.920000   \n",
      "25%                                    174.967500   \n",
      "50%                                    352.605000   \n",
      "75%                                    622.792500   \n",
      "max                                   2680.890000   \n",
      "\n",
      "       hardship_payoff_balance_amount  hardship_last_payment_amount  \\\n",
      "count                    10613.000000                  10613.000000   \n",
      "mean                     11628.036442                    193.606331   \n",
      "std                       7615.161123                    198.694368   \n",
      "min                         55.730000                      0.010000   \n",
      "25%                       5628.730000                     43.780000   \n",
      "50%                      10044.220000                    132.890000   \n",
      "75%                      16114.940000                    284.180000   \n",
      "max                      40306.410000                   1407.860000   \n",
      "\n",
      "       settlement_amount  settlement_percentage  settlement_term  \n",
      "count       33056.000000           33056.000000     33056.000000  \n",
      "mean         5030.606922              47.775600        13.148596  \n",
      "std          3692.027842               7.336379         8.192319  \n",
      "min            44.210000               0.200000         0.000000  \n",
      "25%          2227.000000              45.000000         6.000000  \n",
      "50%          4172.855000              45.000000        14.000000  \n",
      "75%          6870.782500              50.000000        18.000000  \n",
      "max         33601.000000             521.350000       181.000000  \n",
      "\n",
      "[8 rows x 109 columns]\n",
      "Type of data:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#Let´s explore the information in the csv.\n",
    "\n",
    "print('Data obtained with info()\\n', data.info()) # To obtain the infomrmation about the csv columns...\n",
    "\n",
    "print('Data obtained with head()\\n', data.head()) # To obtain the information from the first 5 rows.\n",
    "\n",
    "print('Data obtained with describe()\\n', data.describe()) # To obtain basic statistical information only from numercial columns.\n",
    "\n",
    "print('Type of data: ', type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impressions\n",
    "\n",
    "- There are too many columns, how can we handle them efficiently?\n",
    "\n",
    "- There is not available total information in columns, for example ID and member_id, and that could be a problem, because there is not exits information.\n",
    "\n",
    "# Data not available and duplicated entries.\n",
    "\n",
    "We need to start making decision for see what do with these data, wheter delete it, complete it. The context-appropriate handling.\n",
    "\n",
    "Now let´s see the data not available by NUMERICAL (int64) columns, duplicated rows and null data with the following methods:\n",
    "- .isnull()\n",
    "- .duplicated()\n",
    "- .isna() - Returns boolean information.\n",
    "    - You may combine this method with head() to specify rows or even use ILOC or LOC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not avaialbe by int64 columns: \n",
      " id                       2260668\n",
      "member_id                2260668\n",
      "loan_amnt                      0\n",
      "funded_amnt                    0\n",
      "funded_amnt_inv                0\n",
      "                          ...   \n",
      "settlement_status        2227612\n",
      "settlement_date          2227612\n",
      "settlement_amount        2227612\n",
      "settlement_percentage    2227612\n",
      "settlement_term          2227612\n",
      "Length: 145, dtype: int64 \n",
      "\n",
      "Data duplicated by rows/entries: \n",
      " 0\n",
      "Missing data by column: \n",
      " id                       2260668\n",
      "member_id                2260668\n",
      "loan_amnt                      0\n",
      "funded_amnt                    0\n",
      "funded_amnt_inv                0\n",
      "                          ...   \n",
      "settlement_status        2227612\n",
      "settlement_date          2227612\n",
      "settlement_amount        2227612\n",
      "settlement_percentage    2227612\n",
      "settlement_term          2227612\n",
      "Length: 145, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Data not avaialbe by int64 columns: \\n', data.isnull().sum(), '\\n')\n",
    "\n",
    "print('Data duplicated by rows/entries: \\n', data.duplicated().sum())\n",
    "\n",
    "print('Missing data by column: \\n', data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique values over each column\n",
    "Now we can see the unique values from each column making an iteration for each one, we obtain this with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: id\n",
      "Number of unique values: 1\n",
      "Unique values: [nan]\n",
      "--------------------\n",
      "Column: member_id\n",
      "Number of unique values: 1\n",
      "Unique values: [nan]\n",
      "--------------------\n",
      "Column: loan_amnt\n",
      "Number of unique values: 1572\n",
      "Unique values: [ 2500 30000  5000  4000  5550  2000  6000  5500 28000 11200]\n",
      "--------------------\n",
      "Column: funded_amnt\n",
      "Number of unique values: 1572\n",
      "Unique values: [ 2500 30000  5000  4000  5550  2000  6000  5500 28000 11200]\n",
      "--------------------\n",
      "Column: funded_amnt_inv\n",
      "Number of unique values: 10057\n",
      "Unique values: [ 2500. 30000.  5000.  4000.  5550.  2000.  6000.  5500. 28000. 11200.]\n",
      "--------------------\n",
      "Column: term\n",
      "Number of unique values: 2\n",
      "Unique values: [' 36 months' ' 60 months']\n",
      "--------------------\n",
      "Column: int_rate\n",
      "Number of unique values: 673\n",
      "Unique values: [13.56 18.94 17.97 16.14 15.02 14.47 22.35 11.31  8.19 12.98]\n",
      "--------------------\n",
      "Column: installment\n",
      "Number of unique values: 93296\n",
      "Unique values: [ 84.92 777.23 180.69 146.51 731.78 192.45  72.28 203.79 206.44 211.05]\n",
      "--------------------\n",
      "Column: grade\n",
      "Number of unique values: 7\n",
      "Unique values: ['C' 'D' 'B' 'A' 'E' 'F' 'G']\n",
      "--------------------\n",
      "Column: sub_grade\n",
      "Number of unique values: 35\n",
      "Unique values: ['C1' 'D2' 'D1' 'C4' 'C3' 'C2' 'D5' 'B3' 'A4' 'B5']\n",
      "--------------------\n",
      "Column: emp_title\n",
      "Number of unique values: 512695\n",
      "Unique values: ['Chef' 'Postmaster ' 'Administrative' 'IT Supervisor' 'Mechanic'\n",
      " 'Director COE' 'Account Manager' 'Assistant Director'\n",
      " 'Legal Assistant III' nan]\n",
      "--------------------\n",
      "Column: emp_length\n",
      "Number of unique values: 12\n",
      "Unique values: ['10+ years' '6 years' '4 years' '< 1 year' '2 years' '9 years' nan\n",
      " '5 years' '3 years' '7 years']\n",
      "--------------------\n",
      "Column: home_ownership\n",
      "Number of unique values: 6\n",
      "Unique values: ['RENT' 'MORTGAGE' 'OWN' 'ANY' 'NONE' 'OTHER']\n",
      "--------------------\n",
      "Column: annual_inc\n",
      "Number of unique values: 89369\n",
      "Unique values: [ 55000.  90000.  59280.  92000.  57250. 152500.  51000.  65000.  53580.\n",
      " 300000.]\n",
      "--------------------\n",
      "Column: verification_status\n",
      "Number of unique values: 3\n",
      "Unique values: ['Not Verified' 'Source Verified' 'Verified']\n",
      "--------------------\n",
      "Column: issue_d\n",
      "Number of unique values: 139\n",
      "Unique values: ['Dec-2018' 'Nov-2018' 'Oct-2018' 'Sep-2018' 'Aug-2018' 'Jul-2018'\n",
      " 'Jun-2018' 'May-2018' 'Apr-2018' 'Mar-2018']\n",
      "--------------------\n",
      "Column: loan_status\n",
      "Number of unique values: 9\n",
      "Unique values: ['Current' 'Fully Paid' 'Late (31-120 days)' 'In Grace Period'\n",
      " 'Charged Off' 'Late (16-30 days)' 'Default'\n",
      " 'Does not meet the credit policy. Status:Fully Paid'\n",
      " 'Does not meet the credit policy. Status:Charged Off']\n",
      "--------------------\n",
      "Column: pymnt_plan\n",
      "Number of unique values: 2\n",
      "Unique values: ['n' 'y']\n",
      "--------------------\n",
      "Column: url\n",
      "Number of unique values: 1\n",
      "Unique values: [nan]\n",
      "--------------------\n",
      "Column: desc\n",
      "Number of unique values: 124501\n",
      "Unique values: [nan ' '\n",
      " \"I currently have a loan out with CashCall. The interest rate is 96%! At the time I took out the loan, it helped with a family crisis, but now the interest is crazy to be paying. I'd rather be paying my $200 a month to pay down a loan rather than just the interest.   Also, the remainder of my debt has interest rates ranging from 20 - 24%. This includes credit cards, a student loan, and some personal loans. So, I would like to consolidate the rest of my debt and pay a lower interest rate of 15%.  By doing this, I could lower my interest and my monthly payments and save at least $4000 over the next few years.  I do have a bankruptcy that was discharged 3 years ago due to having $40,000 in debt from an uninsured hospital stay. However, I've not missed a payment on anything since.  My credit report does from time time to time relist things as collections and then I have to dispute them as included in bankruptcy and they go back to being listed that way. I'm not sure why the status gets messed up sometimes.   Thank you for your time and consideration! \"\n",
      " 'Consolidate debt'\n",
      " 'This loan will be used solely to consolidate credit card debts accrued while wife was/is unemployed.'\n",
      " 'I have recently purchased and built a new home that I have always dreamed of having.  I would like to complete the project by putting a hottub in my backyard, however; I am not happy with the rate I  have been offered from GE to finance the spa.  I am paying cash for all other improvements, but this is the final phase and finishing touch to my happiness.  I am in the process of consolidating a lot of debt, and went through a minor period of financial woes, but through determination and hard work I have managed to rebound.  My income is fantastic, and I would pay cash for the item, but would like to reserve the money I have saved for future issues if they should arise.  I am a college graduate, responsible, and work for a very good company that is stable in this very unstable market.  I plan to repay this loan in less than 12 months with a 4th quarter bonus I will be receiving.  Thank you for your consideration.'\n",
      " 'Temporary cash flow challenges. Would like this loan to offset mortgage payments-next 90 days. Have owned current residence 22years and have never been late on a payment.  I am self employed in the same business for 31 years in the same location. Wife works for local school district full time.'\n",
      " 'I am looking to pay off my credit card debts.  '\n",
      " 'Hi! So $5,500 doesn\\'t sound like much to be debt free. Well, when you\\'re 24 years old it seems like quite a bit. I was foolish enough to buy into those \"bad credit auto loans\". Here is what happened.. I bought an old car. I had an extremely high monthly payment. I missed other payments on other accounts due to having to pay for repairs in addition to my monthly payment.  The money will pay off the existing loan, and also pay off some credit card debt.  I will be debt free except for my payment to you.  I don\\'t care what the interest rate is, honestly, as long as it doesn\\'t go above 15% (my current car loan rate).   Let me know if you can help me. I would really appreciate it. '\n",
      " 'I developed poor credit in college due to not having a job and not being able to pay my credit card bills all the time. I am a recent graduate and am employeed with a very large and prestigous contractor. I have a steady income and want to get out of debt. Please help me out. Thank you']\n",
      "--------------------\n",
      "Column: purpose\n",
      "Number of unique values: 14\n",
      "Unique values: ['debt_consolidation' 'credit_card' 'house' 'car' 'other' 'vacation'\n",
      " 'home_improvement' 'small_business' 'major_purchase' 'medical']\n",
      "--------------------\n",
      "Column: title\n",
      "Number of unique values: 63155\n",
      "Unique values: ['Debt consolidation' 'Credit card refinancing' 'Home buying'\n",
      " 'Car financing' 'Other' 'Vacation' 'Home improvement' 'Business'\n",
      " 'Major purchase' 'Medical expenses']\n",
      "--------------------\n",
      "Column: zip_code\n",
      "Number of unique values: 957\n",
      "Unique values: ['109xx' '713xx' '490xx' '985xx' '212xx' '461xx' '606xx' '460xx' '327xx'\n",
      " '068xx']\n",
      "--------------------\n",
      "Column: addr_state\n",
      "Number of unique values: 51\n",
      "Unique values: ['NY' 'LA' 'MI' 'WA' 'MD' 'IN' 'IL' 'FL' 'CT' 'GA']\n",
      "--------------------\n",
      "Column: dti\n",
      "Number of unique values: 10846\n",
      "Unique values: [18.24 26.52 10.51 16.74 26.35 37.94  2.4  30.1  21.16 17.43]\n",
      "--------------------\n",
      "Column: delinq_2yrs\n",
      "Number of unique values: 38\n",
      "Unique values: [ 0.  1.  2.  7.  4.  3.  6.  5.  8. 16.]\n",
      "--------------------\n",
      "Column: earliest_cr_line\n",
      "Number of unique values: 755\n",
      "Unique values: ['Apr-2001' 'Jun-1987' 'Apr-2011' 'Feb-2006' 'Dec-2000' 'Sep-2002'\n",
      " 'Nov-2004' 'Nov-1997' 'Aug-1998' 'Apr-2002']\n",
      "--------------------\n",
      "Column: inq_last_6mths\n",
      "Number of unique values: 29\n",
      "Unique values: [ 1.  0.  3.  2.  4.  5. nan  6.  7.  8.]\n",
      "--------------------\n",
      "Column: mths_since_last_delinq\n",
      "Number of unique values: 174\n",
      "Unique values: [nan 71. 32. 17. 22.  6. 43. 38.  8. 30.]\n",
      "--------------------\n",
      "Column: mths_since_last_record\n",
      "Number of unique values: 130\n",
      "Unique values: [ 45.  75.  nan 100. 107.  67. 106.  60.  92.  62.]\n",
      "--------------------\n",
      "Column: open_acc\n",
      "Number of unique values: 92\n",
      "Unique values: [ 9. 13.  8. 10. 12. 18.  1. 19. 38.  6.]\n",
      "--------------------\n",
      "Column: pub_rec\n",
      "Number of unique values: 44\n",
      "Unique values: [ 1.  0.  3.  2.  4.  5.  6.  8.  7. 15.]\n",
      "--------------------\n",
      "Column: revol_bal\n",
      "Number of unique values: 102251\n",
      "Unique values: [ 4341 12315  4599  5468   829 53854     0 38476  8018 65950]\n",
      "--------------------\n",
      "Column: revol_util\n",
      "Number of unique values: 1431\n",
      "Unique values: [10.3 24.2 19.1 78.1  3.6 48.1  nan 69.3 35.2 49.8]\n",
      "--------------------\n",
      "Column: total_acc\n",
      "Number of unique values: 153\n",
      "Unique values: [34. 44. 13. 26.  9. 37. 38. 58. 23. 27.]\n",
      "--------------------\n",
      "Column: initial_list_status\n",
      "Number of unique values: 2\n",
      "Unique values: ['w' 'f']\n",
      "--------------------\n",
      "Column: out_prncp\n",
      "Number of unique values: 364399\n",
      "Unique values: [ 2386.02 29387.75  4787.21  3831.93 29339.02  5302.5   1914.71  5864.01\n",
      "  4786.79  5730.2 ]\n",
      "--------------------\n",
      "Column: out_prncp_inv\n",
      "Number of unique values: 377353\n",
      "Unique values: [ 2386.02 29387.75  4787.21  3831.93 29339.02  5302.5   1914.71  5864.01\n",
      "  4786.79  5730.2 ]\n",
      "--------------------\n",
      "Column: total_pymnt\n",
      "Number of unique values: 1608336\n",
      "Unique values: [ 167.02 1507.11  353.89  286.71 1423.21  377.95  141.56  201.53  405.64\n",
      "  411.86]\n",
      "--------------------\n",
      "Column: total_pymnt_inv\n",
      "Number of unique values: 1299089\n",
      "Unique values: [ 167.02 1507.11  353.89  286.71 1423.21  377.95  141.56  201.53  405.64\n",
      "  411.86]\n",
      "--------------------\n",
      "Column: total_rec_prncp\n",
      "Number of unique values: 487427\n",
      "Unique values: [113.98 612.25 212.79 168.07 660.98 247.5   85.29 135.99 213.21 269.8 ]\n",
      "--------------------\n",
      "Column: total_rec_int\n",
      "Number of unique values: 629835\n",
      "Unique values: [ 53.04 894.86 141.1  118.64 762.23 130.45  56.27  65.54 140.68 135.84]\n",
      "--------------------\n",
      "Column: total_rec_late_fee\n",
      "Number of unique values: 17991\n",
      "Unique values: [ 0.   15.   35.7  22.   52.99 42.32 27.04 23.04 18.81 21.49]\n",
      "--------------------\n",
      "Column: recoveries\n",
      "Number of unique values: 127920\n",
      "Unique values: [   0.   3322.48 1803.35 1702.51 1904.35 2382.66 1383.25 1891.51 1380.57\n",
      " 5239.76]\n",
      "--------------------\n",
      "Column: collection_recovery_fee\n",
      "Number of unique values: 140449\n",
      "Unique values: [  0.     299.0232 162.3015 153.2259 171.3915 214.4394 124.4925 170.2359\n",
      " 124.2513 471.5784]\n",
      "--------------------\n",
      "Column: last_pymnt_d\n",
      "Number of unique values: 136\n",
      "Unique values: ['Feb-2019' 'Jan-2019' nan 'Dec-2018' 'Nov-2018' 'Oct-2018' 'Sep-2018'\n",
      " 'Aug-2018' 'Jul-2018' 'Jun-2018']\n",
      "--------------------\n",
      "Column: last_pymnt_amnt\n",
      "Number of unique values: 692560\n",
      "Unique values: [ 84.92 777.23 180.69 146.51 731.78 192.45  72.28 208.31 206.44 211.05]\n",
      "--------------------\n",
      "Column: next_pymnt_d\n",
      "Number of unique values: 106\n",
      "Unique values: ['Mar-2019' nan 'Feb-2019' 'Apr-2019' 'Dec-2018' 'Sep-2018' 'Aug-2018'\n",
      " 'Feb-2018' 'Jan-2016' 'Sep-2013']\n",
      "--------------------\n",
      "Column: last_credit_pull_d\n",
      "Number of unique values: 141\n",
      "Unique values: ['Feb-2019' 'Jan-2019' 'Dec-2018' nan 'Nov-2018' 'Oct-2018' 'Sep-2018'\n",
      " 'Aug-2018' 'Jul-2018' 'Jun-2018']\n",
      "--------------------\n",
      "Column: collections_12_mths_ex_med\n",
      "Number of unique values: 17\n",
      "Unique values: [ 0.  1.  2.  4.  3.  5.  6.  8.  9. 10.]\n",
      "--------------------\n",
      "Column: mths_since_last_major_derog\n",
      "Number of unique values: 184\n",
      "Unique values: [nan 45. 22.  6. 48. 68. 35. 37. 52. 43.]\n",
      "--------------------\n",
      "Column: policy_code\n",
      "Number of unique values: 1\n",
      "Unique values: [1]\n",
      "--------------------\n",
      "Column: application_type\n",
      "Number of unique values: 2\n",
      "Unique values: ['Individual' 'Joint App']\n",
      "--------------------\n",
      "Column: annual_inc_joint\n",
      "Number of unique values: 17634\n",
      "Unique values: [    nan 104424.  64793.  59500.  55000.  67590. 176000.  85000. 123500.\n",
      " 239000.]\n",
      "--------------------\n",
      "Column: dti_joint\n",
      "Number of unique values: 4019\n",
      "Unique values: [  nan 10.77 18.91 10.59 34.95 24.54 15.42 27.84 14.53 12.77]\n",
      "--------------------\n",
      "Column: verification_status_joint\n",
      "Number of unique values: 4\n",
      "Unique values: [nan 'Verified' 'Not Verified' 'Source Verified']\n",
      "--------------------\n",
      "Column: acc_now_delinq\n",
      "Number of unique values: 10\n",
      "Unique values: [ 0.  1.  2.  3.  4.  6.  5. 14.  7. nan]\n",
      "--------------------\n",
      "Column: tot_coll_amt\n",
      "Number of unique values: 15575\n",
      "Unique values: [   0. 1208.  686. 6389.  160.   66. 2693.  116.  500.  258.]\n",
      "--------------------\n",
      "Column: tot_cur_bal\n",
      "Number of unique values: 487689\n",
      "Unique values: [ 16901. 321915. 110299. 305049. 116007. 685749.    854.  91535.  41882.\n",
      " 349502.]\n",
      "--------------------\n",
      "Column: open_acc_6m\n",
      "Number of unique values: 20\n",
      "Unique values: [ 2.  4.  0.  1.  3.  5.  6.  7. 13.  8.]\n",
      "--------------------\n",
      "Column: open_act_il\n",
      "Number of unique values: 55\n",
      "Unique values: [ 2.  4.  1.  5.  7.  0.  8.  6.  3. 14.]\n",
      "--------------------\n",
      "Column: open_il_12m\n",
      "Number of unique values: 20\n",
      "Unique values: [ 1.  2.  0.  3.  5.  4.  6.  8.  7. 10.]\n",
      "--------------------\n",
      "Column: open_il_24m\n",
      "Number of unique values: 32\n",
      "Unique values: [ 2.  3.  5.  1.  0.  4.  6.  7.  9. 12.]\n",
      "--------------------\n",
      "Column: mths_since_rcnt_il\n",
      "Number of unique values: 406\n",
      "Unique values: [ 2.  3. 14.  5.  4.  7. 23. 62.  1. 36.]\n",
      "--------------------\n",
      "Column: total_bal_il\n",
      "Number of unique values: 162250\n",
      "Unique values: [ 12560.  87153.   7150.  30683.  28845. 131524.      0.  53059.  33864.\n",
      "  39961.]\n",
      "--------------------\n",
      "Column: il_util\n",
      "Number of unique values: 281\n",
      "Unique values: [69. 88. 72. 68. 89. nan 87. 98. 45. 54.]\n",
      "--------------------\n",
      "Column: open_rv_12m\n",
      "Number of unique values: 30\n",
      "Unique values: [ 2.  4.  0.  1.  3.  5.  6.  9.  7. 18.]\n",
      "--------------------\n",
      "Column: open_rv_24m\n",
      "Number of unique values: 51\n",
      "Unique values: [ 7.  5.  2.  0.  4.  1.  6. 12.  3.  8.]\n",
      "--------------------\n",
      "Column: max_bal_bc\n",
      "Number of unique values: 33727\n",
      "Unique values: [ 2137.   998.     0.  3761.   516. 17584.  9413.  3132. 15926.  2837.]\n",
      "--------------------\n",
      "Column: all_util\n",
      "Number of unique values: 189\n",
      "Unique values: [ 28.  57.  35.  70.  54.  58. 100.  74.  73.  48.]\n",
      "--------------------\n",
      "Column: total_rev_hi_lim\n",
      "Number of unique values: 34221\n",
      "Unique values: [ 42000.  50800.  24100.   7000.  23100. 111900.      0.  55500.  22800.\n",
      " 132500.]\n",
      "--------------------\n",
      "Column: inq_fi\n",
      "Number of unique values: 34\n",
      "Unique values: [1. 2. 0. 3. 6. 5. 7. 4. 8. 9.]\n",
      "--------------------\n",
      "Column: total_cu_tl\n",
      "Number of unique values: 63\n",
      "Unique values: [11. 15.  5.  4.  0.  2.  1.  3.  8.  6.]\n",
      "--------------------\n",
      "Column: inq_last_12m\n",
      "Number of unique values: 49\n",
      "Unique values: [ 2.  0.  3.  6.  1.  4.  5. 10.  7.  9.]\n",
      "--------------------\n",
      "Column: acc_open_past_24mths\n",
      "Number of unique values: 58\n",
      "Unique values: [ 9. 10.  4.  5.  8.  3. 12. 15.  2. 11.]\n",
      "--------------------\n",
      "Column: avg_cur_bal\n",
      "Number of unique values: 88598\n",
      "Unique values: [ 1878. 24763. 18383. 30505.  9667. 40338.   854.  5085.  5235.  9197.]\n",
      "--------------------\n",
      "Column: bc_open_to_buy\n",
      "Number of unique values: 91501\n",
      "Unique values: [34360. 13761. 13800.  1239.  8471. 23746.    nan  3034. 13786. 38683.]\n",
      "--------------------\n",
      "Column: bc_util\n",
      "Number of unique values: 1495\n",
      "Unique values: [ 5.9  8.3  0.  75.2  8.9 64.   nan 90.8 35.9 60.6]\n",
      "--------------------\n",
      "Column: chargeoff_within_12_mths\n",
      "Number of unique values: 12\n",
      "Unique values: [ 0.  1.  4.  2.  3.  7.  5.  6.  9. 10.]\n",
      "--------------------\n",
      "Column: delinq_amnt\n",
      "Number of unique values: 2618\n",
      "Unique values: [   0. 2077. 6113.  158. 5445.  200. 8566.  456. 2265.  413.]\n",
      "--------------------\n",
      "Column: mo_sin_old_il_acct\n",
      "Number of unique values: 567\n",
      "Unique values: [140. 163.  87.  62.  53. 195. 169. 145. 166. 139.]\n",
      "--------------------\n",
      "Column: mo_sin_old_rev_tl_op\n",
      "Number of unique values: 788\n",
      "Unique values: [212. 378.  92. 154. 216. 176.  40. 253. 244. 200.]\n",
      "--------------------\n",
      "Column: mo_sin_rcnt_rev_tl_op\n",
      "Number of unique values: 334\n",
      "Unique values: [ 1.  4. 15. 64.  2. 10. 23. 13.  6.  5.]\n",
      "--------------------\n",
      "Column: mo_sin_rcnt_tl\n",
      "Number of unique values: 233\n",
      "Unique values: [ 1.  3. 14.  5.  2.  4.  7. 13.  6. 27.]\n",
      "--------------------\n",
      "Column: mort_acc\n",
      "Number of unique values: 48\n",
      "Unique values: [0. 3. 2. 6. 1. 4. 5. 9. 8. 7.]\n",
      "--------------------\n",
      "Column: mths_since_recent_bc\n",
      "Number of unique values: 547\n",
      "Unique values: [ 1.  4. 77. 64.  2. 20. nan 14.  6.  5.]\n",
      "--------------------\n",
      "Column: mths_since_recent_bc_dlq\n",
      "Number of unique values: 178\n",
      "Unique values: [nan 33. 22. 48. 35. 37. 52. 43. 57. 71.]\n",
      "--------------------\n",
      "Column: mths_since_recent_inq\n",
      "Number of unique values: 27\n",
      "Unique values: [ 2.  4. 14.  5. 13.  3.  1.  6. 15.  7.]\n",
      "--------------------\n",
      "Column: mths_since_recent_revol_delinq\n",
      "Number of unique values: 180\n",
      "Unique values: [ nan  32.  17.  22. 117.   8.  30.  48.  37.  52.]\n",
      "--------------------\n",
      "Column: num_accts_ever_120_pd\n",
      "Number of unique values: 45\n",
      "Unique values: [ 0.  2.  1.  7.  6.  3.  4.  5. 12.  8.]\n",
      "--------------------\n",
      "Column: num_actv_bc_tl\n",
      "Number of unique values: 43\n",
      "Unique values: [ 2.  0.  1.  4.  7. 16.  3.  5.  6.  8.]\n",
      "--------------------\n",
      "Column: num_actv_rev_tl\n",
      "Number of unique values: 58\n",
      "Unique values: [ 5.  4.  3.  2.  6.  0. 12. 20. 19.  9.]\n",
      "--------------------\n",
      "Column: num_bc_sats\n",
      "Number of unique values: 61\n",
      "Unique values: [ 3.  4.  1.  6.  0.  8.  5. 19.  2.  7.]\n",
      "--------------------\n",
      "Column: num_bc_tl\n",
      "Number of unique values: 77\n",
      "Unique values: [ 3.  9.  2.  8. 10. 26.  4.  5.  1.  7.]\n",
      "--------------------\n",
      "Column: num_il_tl\n",
      "Number of unique values: 123\n",
      "Unique values: [16. 27.  4.  7.  9. 23.  5. 15. 20. 13.]\n",
      "--------------------\n",
      "Column: num_op_rev_tl\n",
      "Number of unique values: 82\n",
      "Unique values: [ 7.  8.  6.  2.  9.  0. 14. 33.  3. 10.]\n",
      "--------------------\n",
      "Column: num_rev_accts\n",
      "Number of unique values: 118\n",
      "Unique values: [18. 14.  7.  3. 15. 20. 48.  5.  9. 10.]\n",
      "--------------------\n",
      "Column: num_rev_tl_bal_gt_0\n",
      "Number of unique values: 51\n",
      "Unique values: [ 5.  4.  3.  2.  7.  0. 12. 20. 19.  6.]\n",
      "--------------------\n",
      "Column: num_sats\n",
      "Number of unique values: 92\n",
      "Unique values: [ 9. 13.  8. 10. 12. 18.  1. 19. 38.  6.]\n",
      "--------------------\n",
      "Column: num_tl_120dpd_2m\n",
      "Number of unique values: 8\n",
      "Unique values: [ 0. nan  1.  2.  4.  3.  6.  7.]\n",
      "--------------------\n",
      "Column: num_tl_30dpd\n",
      "Number of unique values: 6\n",
      "Unique values: [ 0.  1.  2.  3.  4. nan]\n",
      "--------------------\n",
      "Column: num_tl_90g_dpd_24m\n",
      "Number of unique values: 35\n",
      "Unique values: [ 0.  1.  2.  4.  5.  6. 16.  9.  3.  7.]\n",
      "--------------------\n",
      "Column: num_tl_op_past_12m\n",
      "Number of unique values: 34\n",
      "Unique values: [3. 6. 0. 5. 4. 2. 1. 7. 9. 8.]\n",
      "--------------------\n",
      "Column: pct_tl_nvr_dlq\n",
      "Number of unique values: 691\n",
      "Unique values: [100.   95.   92.3  78.9  84.6  90.2  76.   80.   86.5  77.8]\n",
      "--------------------\n",
      "Column: percent_bc_gt_75\n",
      "Number of unique values: 285\n",
      "Unique values: [  0.  100.   60.    nan  85.7  26.3  37.5  20.   50.   33.3]\n",
      "--------------------\n",
      "Column: pub_rec_bankruptcies\n",
      "Number of unique values: 13\n",
      "Unique values: [1. 0. 3. 2. 4. 5. 6. 7. 8. 9.]\n",
      "--------------------\n",
      "Column: tax_liens\n",
      "Number of unique values: 43\n",
      "Unique values: [ 0.  1.  2.  4.  6.  3.  5.  8.  7. 15.]\n",
      "--------------------\n",
      "Column: tot_hi_cred_lim\n",
      "Number of unique values: 529973\n",
      "Unique values: [ 60124. 372872. 136927. 385183. 157548. 831687.    854. 117242.  57426.\n",
      " 477390.]\n",
      "--------------------\n",
      "Column: total_bal_ex_mort\n",
      "Number of unique values: 212778\n",
      "Unique values: [ 16901.  99468.  11749.  36151.  29674. 185378.    854.  91535.  41882.\n",
      " 105911.]\n",
      "--------------------\n",
      "Column: total_bc_limit\n",
      "Number of unique values: 20310\n",
      "Unique values: [36500. 15000. 13800.  5000.  9300. 65900.     0. 33100. 21500. 98300.]\n",
      "--------------------\n",
      "Column: total_il_high_credit_limit\n",
      "Number of unique values: 194138\n",
      "Unique values: [ 18124.  94072.  10000.  44984.  32332. 203159.      0.  61742.  34626.\n",
      "  89600.]\n",
      "--------------------\n",
      "Column: revol_bal_joint\n",
      "Number of unique values: 56876\n",
      "Unique values: [   nan 10286. 24654.  6902. 29704. 32918. 19599. 34437.  7980. 64367.]\n",
      "--------------------\n",
      "Column: sec_app_earliest_cr_line\n",
      "Number of unique values: 664\n",
      "Unique values: [nan 'May-2002' 'Aug-2003' 'May-2003' 'Oct-1999' 'Dec-1996' 'May-2007'\n",
      " 'Oct-2006' 'Nov-1996' 'Jul-1997']\n",
      "--------------------\n",
      "Column: sec_app_inq_last_6mths\n",
      "Number of unique values: 8\n",
      "Unique values: [nan  2.  0.  1.  4.  3.  5.  6.]\n",
      "--------------------\n",
      "Column: sec_app_mort_acc\n",
      "Number of unique values: 24\n",
      "Unique values: [nan  3.  2.  0.  1.  4.  6.  7.  5. 10.]\n",
      "--------------------\n",
      "Column: sec_app_open_acc\n",
      "Number of unique values: 68\n",
      "Unique values: [nan  2.  9.  6. 16. 17.  3. 10. 14. 24.]\n",
      "--------------------\n",
      "Column: sec_app_revol_util\n",
      "Number of unique values: 1217\n",
      "Unique values: [ nan 44.1 78.9 47.9 48.8 92.  95.7 71.8 26.3 60.9]\n",
      "--------------------\n",
      "Column: sec_app_open_act_il\n",
      "Number of unique values: 41\n",
      "Unique values: [nan  0.  1.  6.  3.  2.  5.  7. 16.  8.]\n",
      "--------------------\n",
      "Column: sec_app_num_rev_accts\n",
      "Number of unique values: 87\n",
      "Unique values: [nan  6. 12. 21. 15. 11.  5.  9. 10. 19.]\n",
      "--------------------\n",
      "Column: sec_app_chargeoff_within_12_mths\n",
      "Number of unique values: 23\n",
      "Unique values: [nan  0. 10.  1.  2.  3.  8.  4.  5.  6.]\n",
      "--------------------\n",
      "Column: sec_app_collections_12_mths_ex_med\n",
      "Number of unique values: 19\n",
      "Unique values: [nan  0.  2. 10.  1.  3.  4.  5.  8.  6.]\n",
      "--------------------\n",
      "Column: sec_app_mths_since_last_major_derog\n",
      "Number of unique values: 141\n",
      "Unique values: [nan 46. 25. 37. 70. 64. 80. 20. 49. 35.]\n",
      "--------------------\n",
      "Column: hardship_flag\n",
      "Number of unique values: 2\n",
      "Unique values: ['N' 'Y']\n",
      "--------------------\n",
      "Column: hardship_type\n",
      "Number of unique values: 2\n",
      "Unique values: [nan 'INTEREST ONLY-3 MONTHS DEFERRAL']\n",
      "--------------------\n",
      "Column: hardship_reason\n",
      "Number of unique values: 10\n",
      "Unique values: [nan 'UNEMPLOYMENT' 'NATURAL_DISASTER' 'EXCESSIVE_OBLIGATIONS' 'MEDICAL'\n",
      " 'INCOME_CURTAILMENT' 'DISABILITY' 'REDUCED_HOURS' 'FAMILY_DEATH'\n",
      " 'DIVORCE']\n",
      "--------------------\n",
      "Column: hardship_status\n",
      "Number of unique values: 4\n",
      "Unique values: [nan 'ACTIVE' 'COMPLETED' 'BROKEN']\n",
      "--------------------\n",
      "Column: deferral_term\n",
      "Number of unique values: 2\n",
      "Unique values: [nan  3.]\n",
      "--------------------\n",
      "Column: hardship_amount\n",
      "Number of unique values: 8951\n",
      "Unique values: [   nan 378.39 203.67 378.9  126.27 119.59 243.56 159.94 241.9  134.81]\n",
      "--------------------\n",
      "Column: hardship_start_date\n",
      "Number of unique values: 27\n",
      "Unique values: [nan 'Feb-2019' 'Oct-2018' 'Nov-2018' 'Sep-2018' 'Jan-2019' 'Dec-2018'\n",
      " 'Aug-2018' 'Jul-2018' 'May-2018']\n",
      "--------------------\n",
      "Column: hardship_end_date\n",
      "Number of unique values: 28\n",
      "Unique values: [nan 'Apr-2019' 'Dec-2018' 'Jan-2019' 'Feb-2019' 'Oct-2018' 'May-2019'\n",
      " 'Mar-2019' 'Nov-2018' 'Aug-2018']\n",
      "--------------------\n",
      "Column: payment_plan_start_date\n",
      "Number of unique values: 27\n",
      "Unique values: [nan 'Feb-2019' 'Oct-2018' 'Nov-2018' 'Dec-2018' 'Jan-2019' 'Sep-2018'\n",
      " 'Mar-2019' 'Aug-2018' 'Jun-2018']\n",
      "--------------------\n",
      "Column: hardship_length\n",
      "Number of unique values: 2\n",
      "Unique values: [nan  3.]\n",
      "--------------------\n",
      "Column: hardship_dpd\n",
      "Number of unique values: 35\n",
      "Unique values: [nan 22.  0. 18. 23. 28. 29.  7. 16. 11.]\n",
      "--------------------\n",
      "Column: hardship_loan_status\n",
      "Number of unique values: 6\n",
      "Unique values: [nan 'Late (16-30 days)' 'Issued' 'Current' 'Late (31-120 days)'\n",
      " 'In Grace Period']\n",
      "--------------------\n",
      "Column: orig_projected_additional_accrued_interest\n",
      "Number of unique values: 7314\n",
      "Unique values: [    nan 1135.17  611.01 1136.7   378.81  358.77  730.68  479.82  725.7\n",
      "  404.43]\n",
      "--------------------\n",
      "Column: hardship_payoff_balance_amount\n",
      "Number of unique values: 10592\n",
      "Unique values: [     nan 15351.85 40149.35 23090.22  8096.8   9649.7  12701.26 19071.77\n",
      " 19495.24  9674.48]\n",
      "--------------------\n",
      "Column: hardship_last_payment_amount\n",
      "Number of unique values: 8796\n",
      "Unique values: [        nan 1.04541e+03 1.01000e+00 1.21000e+00 2.92830e+02 3.46430e+02\n",
      " 1.00000e-01 1.90000e-01 4.70130e+02 2.00000e-01]\n",
      "--------------------\n",
      "Column: disbursement_method\n",
      "Number of unique values: 2\n",
      "Unique values: ['Cash' 'DirectPay']\n",
      "--------------------\n",
      "Column: debt_settlement_flag\n",
      "Number of unique values: 2\n",
      "Unique values: ['N' 'Y']\n",
      "--------------------\n",
      "Column: debt_settlement_flag_date\n",
      "Number of unique values: 83\n",
      "Unique values: [nan 'Feb-2019' 'Dec-2018' 'Jan-2019' 'Nov-2018' 'Oct-2018' 'Sep-2018'\n",
      " 'Aug-2018' 'Jul-2018' 'Jun-2018']\n",
      "--------------------\n",
      "Column: settlement_status\n",
      "Number of unique values: 4\n",
      "Unique values: [nan 'ACTIVE' 'COMPLETE' 'BROKEN']\n",
      "--------------------\n",
      "Column: settlement_date\n",
      "Number of unique values: 90\n",
      "Unique values: [nan 'Feb-2019' 'Dec-2018' 'Jan-2019' 'Nov-2018' 'Oct-2018' 'Sep-2018'\n",
      " 'Aug-2018' 'Jul-2018' 'Jun-2018']\n",
      "--------------------\n",
      "Column: settlement_amount\n",
      "Number of unique values: 21520\n",
      "Unique values: [   nan  5443. 10119. 23506.  3422. 13175.  5180.  1059. 14333.  5694.]\n",
      "--------------------\n",
      "Column: settlement_percentage\n",
      "Number of unique values: 2046\n",
      "Unique values: [  nan 65.   65.01 60.   64.97 60.01 64.99 50.   45.   47.16]\n",
      "--------------------\n",
      "Column: settlement_term\n",
      "Number of unique values: 41\n",
      "Unique values: [nan 18.  6. 14. 12. 24.  1. 16. 17.  3.]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "unique_values = {\n",
    "    col : data[col].unique() for col in data.columns\n",
    "}\n",
    "\"\"\"\n",
    "We create a dictionary such that its keys are all the columns called col, then we iterate over each column to obtain its unique values\n",
    "\"For each column into the columns from the dataset, obtain its unique value\"\n",
    "\n",
    "In PANDAS, the notation to specify a column is with [], dataset[column]\n",
    "\"\"\"\n",
    "\n",
    "# Now let´s start visualizing these values\n",
    "\n",
    "for col, values in unique_values.items():\n",
    "    print(f'Column: {col}')\n",
    "    print(f'Number of unique values: {len(values)}')\n",
    "    print(f'Unique values: {values[:10]}') # We obtain the first 10 values\n",
    "    print('-'*20) # Separation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "Notice that the dataset have already None into the empty cells, with this applied is easier work the entire dataset.\n",
    "\n",
    "Now our homework is check if we need to make changes into specific columns that we will handle or not. \n",
    "\n",
    "\n",
    "Entire columns without data:\n",
    "    - Member ID, We may relation its values with another column that satisfies a condition. \n",
    "    Even this could be the prumary key of another table, which relates to... another thing for example another loan request.\n",
    "\n",
    "    - ID, this will be our primary key for SQL, if we are only use Pandas to handle information it maybe not be neccesary, however, for this case we will use SQL and another observation is that teh orignal dataset does not have an idenitifier! That could be a problem **so at very least** we are adding a new identifier.\n",
    "\n",
    "    - URL\n",
    "Handle them using SQL\n",
    "\n",
    "- There is not problem with the column 'desc' this is a description provided by the customer. We can handle it later.\n",
    "- mths_since_last_delinq, fill with zeros the gap cells.\n",
    "- Check 'mths_since_last_record' and 'revol_util'\n",
    "\n",
    "\n",
    "\n",
    "### Handling date columns.\n",
    "We are going to fill the columns with \"none\" because we will plot each column or the correspondent handling and see how fill the gaps, if we can interpolet them or given a determinated value and see whether is important assign a value or not.\n",
    "This task we may do it with R.\n",
    "\n",
    "**Remember, you have to create a new dataframe before start cleaning the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Belongs to Cleansing Data:\n",
    "data_cleaned = data.copy()\n",
    "\n",
    "\n",
    "#Performace \n",
    "# Column Member ID\n",
    "data_cleaned['mths_since_last_record'] = data_cleaned['mths_since_last_record'].fillna(0)\n",
    "data_cleaned['id'] = range(1, len(data_cleaned)+1)\n",
    "data_cleaned['mths_since_last_delinq'] = data_cleaned['mths_since_last_delinq'].fillna(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not available in specific column:  0\n",
      "Porcentage of not aveable data: \n",
      " id                         0.000000\n",
      "member_id                100.000000\n",
      "loan_amnt                  0.000000\n",
      "funded_amnt                0.000000\n",
      "funded_amnt_inv            0.000000\n",
      "                            ...    \n",
      "settlement_status         98.537777\n",
      "settlement_date           98.537777\n",
      "settlement_amount         98.537777\n",
      "settlement_percentage     98.537777\n",
      "settlement_term           98.537777\n",
      "Length: 145, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "print('Data not available in specific column: ', data_cleaned['funded_amnt'].isna().sum())\n",
    "print('Porcentage of not aveable data: \\n', (data_cleaned.isna().sum() / len(data_cleaned)) * 100, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group by - We are still getting to know our dataset.\n",
    "## Single groups\n",
    "There is another function useful for these cases to obtain the unique values from a column, this method counts how many times each unique value appears in the defined column. \n",
    "- variable = df['Column']. value_counts()\n",
    "\n",
    "_It is **stored** into a variable, it does not create a new dataframe!_\n",
    "\n",
    "We review this method later when we handling columns.\n",
    "\n",
    "We can also perform operations with these grouped values:\n",
    "- variable = df.groupby('ColumnToGroup')['ColumnToApplyOperations'].sum() at the end the operation\n",
    "\n",
    "If we want, we may add operations for each unique value, for example the sum and mean:\n",
    "- variable = df.groupby(['ColumnToGroup])['ColumnToApplyOpereations].agg(['mean', 'sum'])\n",
    "**Notice the syntax is different from the previous application with a single operation, for this case we use another method called .agg() and the parameter is a list of operations to apply.**\n",
    "\n",
    "## We can also make groups of grouped data with the same method .gropuby()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At this point, you have already known your dataset.\n",
    "\n",
    "#  Data cleansing\n",
    "We need to perform data cleaning before stating the analysis. WE NEED TO CREATE A NEW DATAFRAME FOR THAT.\n",
    "\n",
    "This we will make it with the following methods:\n",
    "- drop.duplicates()\n",
    "But for this datset there are not duplicated rows.\n",
    "\n",
    "## Be careful with the method _dropna()_ it is more strict with rows even if you specify columns.\n",
    "If we use the method _dropna()_ all the rows with at least one empty cell could be removed! \n",
    "And the same for columns if you add the parameter _axis=1_ !\n",
    "- dropna() using the parameter \"subset\" to specify a column or columns, remember the notation ['name']\n",
    "    - dropna(['Column1', 'Column2']) **This REMOVE THE ROWS where there is not information into the specified columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be careful with data not available - Cleaning the information\n",
    "- For this case, if we use dropna() all the dataframe will be deleted because there are too many gaps (espacios vacios/faltantes) in all the rows.\n",
    "\n",
    "- _We could fill these cells or fill only the columns we will working with_\n",
    "    - .fillna(x)\n",
    "\n",
    "- There could be a problem with the operations whether we fill numerical columns with zeros, be careful with that.\n",
    "    - **Perform** an operation with filled and unfilled columns, check their respective outcomes (envolved variables: data, data_cleaned).\n",
    "\n",
    "- For some cases we may fill this missing information with data statistic specifying the column.\n",
    "    - mean = data['Column'].mean()\n",
    "    data = data['Column'].fillna(mean)\n",
    "\n",
    "## Use \"none\" value for empty data, be careful.\n",
    "If there are none values into the empty data from the columns, in the sum this does not have value it is 0, and for counting this are omitted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At this point, you have already cleaned your dataset.\n",
    "\n",
    "# Data analysis \n",
    "**Check if you need to creat another dataframe (as we created for data cleansing) or just specify the colums to use**.\n",
    "I may divide the dataset into category datasets for example, the columns that give information about where live the customers, a classification by its incomes and relationed columns, their delinquencies, etc., to handle better all the dataset without exeptions. \n",
    "\n",
    "\n",
    "## Create DataFrames from a csv (or Series - Unidimensional, similar to an one column)\n",
    "\n",
    "**__Divide your information into portions for a better analysis, cleaning or extracting of information, it all depends of what you want to do.__**\n",
    "\n",
    "Example, **we will create a bidimensional length dataframe with numpy and pandas**\n",
    "    data = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "    df_from_array = pd.DataFrame(data, columns=['A','B','C']) These are the name of columns.\n",
    "\n",
    "There is another example creating from dictionaries.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Take SAMPLES from the dataset to create smaller SUBSETS using ILOC!\n",
    "- Use ILOC \n",
    "    - .iloc[: , :] The first index is for rows and the following for columns.\n",
    "    - .iloc[] For specific rows\n",
    "    - .iloc[x,y] For accesing cells\n",
    "\n",
    "## Knowing the information of specific ROWS OR columns.\n",
    "- Use LOC\n",
    "    - .loc[x]\n",
    "    - .loc[:] For a chuck of rows\n",
    "    - .loc[:, ['Column']]\n",
    "    - .loc[: , ['Column1', 'Column2', 'Column3']] Notice that the second parameter is a list, and each column is inside it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## What you want to solve?\n",
    "What can we obtain from this dataset that could help in the development of the club?\n",
    "\n",
    "- We can compute the expected value for some columns, we need to check each one and see if it is useful for us.\n",
    "- Date more common where customers apply for a debt.\n",
    "\n",
    "A customer could obtain a credit from the club? Can apply?\n",
    "- I can make a filter for some columns that indicates if the customer has checks-offs, deliquencies, ratios specifics that contain the dataset.\n",
    "- Co-borrowers could mean less risk\n",
    "\n",
    "\n",
    "\n",
    "## We will use specific columns, do not need to use the 150 columns at the same time.\n",
    "Annual incomes provided by customers\n",
    "- We can plot the income amounts of customers.\n",
    "- Use statistic tools like quartiles for that specific column.\n",
    "\n",
    "## Analyze an specific column and row:\n",
    "- print(data.['Column'][1]) - I can save this into a variable too.\n",
    "- column_analyzed = data.['Column'].mean() - This previous method may change to our preference.\n",
    "- .count() uses the same syntax. **Always be careful with null values for every calculation**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Make operations between columns.\n",
    "What new columns can we create, add, operate on, or even transform their data type?\n",
    "For example:\n",
    "- Make operation with each other: \n",
    "    - data_cleaned['NewColumn'] = data_cleaned['Column1'] * data_cleaned['Column2']\n",
    "\n",
    "- Change the data type of one column: \n",
    "    - data_cleaned['ColumnHandle'] = pd.to_datetime(data_cleaned['ColumnHandle'])\n",
    "    We can check the changes with the method info().\n",
    "\n",
    "-Divide the information from a column with a data type specific, usually with datetime data:\n",
    "    - data_cleaned['SubColumns'] = data_cleaned['DatatimeColumn'].dt.year, for example\n",
    "### Handling colummns, creating new statistical or operational columns, this is the point.\n",
    "\n",
    "This led us to the following information cell. How can I handle the columns whether I want to make operations between them? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling columns \n",
    "We keep using Pandas, let us define df as the DataFrame we are working with.\n",
    "- Platform operations between columns\n",
    "    - df['NewColumn'] = df.['Column1'] **Operation** df.['Column2']\n",
    "\n",
    "- Boolean values specifying something of our interest.\n",
    "    - df['NewColumn'] = df.['Column1'] **Condition**\n",
    "\n",
    "# Them of create the sub-datasets we are will working with...\n",
    "- Know the data type with method info(), you need notice that there are not non-null data in each one.\n",
    "\n",
    "## Changing the data type:\n",
    "We will change the information of the same column, that is why we call the column and them we apply the changes to the same column.\n",
    "- df['Column'] = pd.to_datetime/type(df['Column'])\n",
    "\n",
    "## Lambda\n",
    "We can also create new columns applying lambda funtions **using the method apply()**\n",
    "    - df['NewColumn'] = df['Column'].apply(lambda x : _operation_). The operation defines the cell´s information.\n",
    "\n",
    "## Normal Functions\n",
    "We also have to use the method apply(), we define a functions as we always do in python, there is not difference.\n",
    "    - Create your functions with conditionals or whatever.\n",
    "    - Use the same syntax as lamda functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan Amount Ststistic Description:\n",
      " count    2.260668e+06\n",
      "mean     1.504693e+04\n",
      "std      9.190245e+03\n",
      "min      5.000000e+02\n",
      "25%      8.000000e+03\n",
      "50%      1.290000e+04\n",
      "75%      2.000000e+04\n",
      "max      4.000000e+04\n",
      "Name: loan_amnt, dtype: float64\n",
      "Loan Amount Ststistic Description:\n",
      " count    2.260668e+06\n",
      "mean     1.504166e+04\n",
      "std      9.188413e+03\n",
      "min      5.000000e+02\n",
      "25%      8.000000e+03\n",
      "50%      1.287500e+04\n",
      "75%      2.000000e+04\n",
      "max      4.000000e+04\n",
      "Name: funded_amnt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting to know still the information:\n",
    " \n",
    "print('Loan Amount Ststistic Description:\\n', data_cleaned['loan_amnt'].describe()) \n",
    "print('Loan Amount Ststistic Description:\\n', data_cleaned['funded_amnt'].describe()) #The total amount committed to that loan at that point in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "There is not problem using the describe() method, if we only want to check the amounts, it may be more appropiated to start plotting our data, I think that we are still knowing our data. For this case, I think the correspondent graph for this column is a boxplot.\n",
    "\n",
    "But if you cannot interpretate a boxplot, well, we can still using the numerical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kz/rybrs64563zc2gl94rf00qbw0000gn/T/ipykernel_4885/1985240455.py:3: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot(data['funded_amnt'], patch_artist=False, notch=False, vert=False, labels=['Loan Amount'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x30cc38ac0>,\n",
       "  <matplotlib.lines.Line2D at 0x30cc38d60>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x30cc38fd0>,\n",
       "  <matplotlib.lines.Line2D at 0x30cc0c2e0>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x30cc38820>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x30cc0c580>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x30cc0c820>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.boxplot(data['funded_amnt'], patch_artist=False, notch=False, vert=False, labels=['Loan Amount'])\n",
    "\n",
    "# Repetimos nombrar nuestros elementos y mostramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv',\n",
       "       'term', 'int_rate', 'installment', 'grade', 'sub_grade',\n",
       "       ...\n",
       "       'hardship_payoff_balance_amount', 'hardship_last_payment_amount',\n",
       "       'disbursement_method', 'debt_settlement_flag',\n",
       "       'debt_settlement_flag_date', 'settlement_status', 'settlement_date',\n",
       "       'settlement_amount', 'settlement_percentage', 'settlement_term'],\n",
       "      dtype='object', length=145)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the information.\n",
    "Which columns may I use for this point?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
