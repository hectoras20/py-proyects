\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath} 
%%
\usepackage[margin=1in]{geometry}

%Esto es para las lineas de "Paradoja de la Urna - Probabilidad I"
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[L]{Notes}
\fancyhead[R]{Linear Algebra I}
\fancyfoot[C]{\thepage}
%%

\title{Linear Algebra}
\author{Héctor Astudillo}
\date{December 2024}

\begin{document}

\maketitle

\section*{Section 1.1 - Vector Spaces}
\subsection*{Vectors}
Let \( F \) be a field. A vector space over \( F \) (denoted by \( EV/F \)) is a set \( V \) (\textbf{whose} elements are vectors that belonging to the vector space), equipped with two operations that satisfy 8 axioms:

\begin{itemize}
    \item \(EV1\) Commutativity of addition: 
    Let \( u, v \in V \), then \( u + v \in V \).
    \item \(EV2\) Associativity of addition:
    For all \( u, v, w \in V \), we have that \( u + (v + w) = (u + v) + w \).
    \item Commutativity of addition: 
    For all \( u, v \in V \), we have that \( u + v = v + u \).
    \item \(EV3\) Elemento neutro aditivo - Additive identity: 
    Exists \( 0 \in V \) such that \( u + 0 = u \) para todo \( u \in V \).
    \item \(EV4\) Elemento inverso aditivo - Additive inverses:
    For each \( u \in V \), exist a unique element \( -u \in V \) such that \( u + (-u) = 0 \).
    \item \(EV5\) Scalar Product/Multiplication: 
    For all \( c, d \in F \) and \( u \in V \), it satisfies that \( (c \cdot d) \cdot u = c \cdot (d \cdot u) \).
    \item \(EV6\) Neutro/Identidad escalar - Identity element of Scalar Multiplication: 
    For all \( u \in V \), it follows that \( 1 \cdot u = u \), where \( 1 \) is the identity element of the product from the field \( F \).
    \item \(EV7\) Distributivity of scalar over field addition: 
    For all \( c, d \in F \) and \( u \in V \), it holds that \( (c + d) \cdot u = c \cdot u + d \cdot u \).
    \item \(EV8\) Distributivity of scalar over vector addition: 
    For all \( c \in F \) y \( u, v \in V \), se cumple \( c \cdot (u + v) = c \cdot u + c \cdot v \).
\end{itemize}

\(F^n\) means n-tuples (adas) of numbers from the field \(F\)

For example, if \(x \in F^n\), then we are talking about a vector with \(n\) entries, whose coefficients belon tot the field \(F\).
\\
\subsection*{Matrices}
Let \(m,n \in N\) y \(F\) a field, we define \(F^{mxn}\) as the set/collection of all \(m x n\) matrices with entries from \(F\)
\\
\\
Remember that:
\\
The sum of matrices is equal entry by entry.
\\
\\
Let us formally define the addition and scalar product on the vectorial space \(F^{mxn}\).
\\

Addition:

Let \(M, N \in F\)...

For every \(i \leq m\) and every \(j \leq m\) we have that \((M + N)_{ij} = M_{ij} + N_{ij}\)
\\

Scalar Product

\(\alpha \in F\), \(m \in F^{mxn}\) such that \(\alpha M \in F^{mxn}\)

Then, \(i \leq m\) and every \(j \leq m\) we have that \((M + N)_{ij} := \alpha \cdot M_{ij}\)
\\
\\
Now we can see that this kind of spaces satisfy the remaining 6 axioms.
\\
\\
Another example... now, we are going to work with functions.

\subsection*{Functions}
Let \(F\) be a field and \(S\) \textbf{a non-empty set}
\\
We define \(F^S\) as \((f | f: S \rightarrow F)\)
\\
In this case, \(S\) is the codomain and \(F\) (the field) is the domain.
\\

Notation:

We can express a function in 2 different ways...
\begin{itemize}
    \item \(f : [a,b] \rightarrow R\)
    \item \(F \in R^{[a,b]}\)
\end{itemize}

Remember the following from Calculus 1:

Let \(f,g \in R^{[a,b]}\) the addition of this functions \(f+g\) maps from \([a,b]\) to \( \mathbb{R}, \forall x \in [a,b]\)

Then, \((f+g)(x) = f(x) + g(x)\)
\\
\\
So it´s the same for our example, now we are going to define the scalar product:

Let \(\alpha \in F\) and \(f \in F^S\)

We want to get \(\alpha \cdot f \in F^S\)

\(\forall t \in S\)

\((\alpha \cdot f)(t) := \alpha \cdot f(t)\)
\\
\\
We already have the addition and scalar product now we can compute the remaining 6 axioms, but there is something else if we want to prove that a functions is equal to another function, we need to prove that the codomain, domain and rule of correspondence it´s the same for both.
\\

Lets shall the \(EV8\)

Let \(\alpha \in F\) and \(f,g \in F^S\)

PD. \(\alpha \cdot (f+g) = \alpha f +\alpha g\)
\\

Domain:

\(dom(\alpha(f+g)) = S\)

\(dom(\alpga f + \alpha g) = S\)
\\

Codomain:

\(cod(\alpha(f+g)) = F\)

\(cod(\alpga f + \alpha g) = F\)
\\

For the rule of correspondence:

Let \(t \in S\)

\((\alpha(f+g))(t) = \alpha \cdot (f+g)(t)\)

\(=\alpha \cdot (f(t) + g(t))\) the field has distributivity

\(= \alpha \cdot f(t) + \alpha \cdot g(t)\)

\(=(\alpha f)(t) + (\alpha g)(t)\)

\(=(\alpha f + \alpha g)(t)\)

\subsection*{Polynomials}

A polynomial with coeficients in \(F\) is:

\[
\sum_{i=0}^{n} a_i x^i = a_0 + a_1 x^1 + a_2x^2 + a_3x^3... +a_nx^n} 
\] 
where \(n \in \mathbb{N} \cup {0}\) and \(\{a_i : 0 \leq i \leq n\} \subsetneq F\)
\\
\\
\(F[x]\) is the collection of all the polynomials with coefficients in \(F\)
\\
\\
Now we need to define the addition and scalar product to shall the remaining 6 axioms. Let´s see how to define formally a polynomial:

Let \(p \in F[x] \rightarrow \exists n \in \mathbf{N}\cup\{0\}\)


\(\qquad \qquad \qquad \quad \exists \{a_i : 0\leq i \leq n\} \subset F\)

Such that 

\[ \(p = \sum_{i=0}^{n} a_ix^i\). \]

The same if we have \(q \in F[x]\)

Let \(q \in F[x] \rightarrow \exists m \in \mathbf{N}\cup\{0\}\)


\(\qquad \qquad \qquad \quad \exists \{b_i : 0\leq i \leq m\} \subset F\)

Such that 

\[ \(q = \sum_{i=0}^{m} b_ix^i\). \]
\\
\textbf{Notice that each polynomial has its own highest degree term and set of coefficients}
\\
The degree term n and m could be any value in Natural field, anyone, so one could be bigger than other, that is the reason why we define \(k\) 

Let \(k := max\{m,n\} \rightarrow \forall n < i \leq k\) \((a_i :=0)\)

\(\qquad \qquad \qquad \qquad \qquad \forall m < i \leq k\) \((b_i :=0)\)

This help us to complete, force the polynomial to have the same degree term (the highest)
\\

Let´s shall and make an abstract definition of addition and scalar product with polynomials and then, prove the remaining 8 axioms.
\\
\\
Addition
\\
We are going to use \(k\) the highest value between m and n that complete the polynomial.

Let \(p, q \in F[X]\)

We define the addition of polynomials as follow:
\[
\(p+ q = \sum_{i=0}^{k} (a_i + b_i) x^i\) \]
\\
\\
Scalar Product

Let \(\alpha \in F\) and \(p \in F[x]\)

Remember how to define formally an polynomial:

\(p \in F(x) \rightarrow \exists n \in \mathbf{N}\cup \{0\}\)

\(\qquad \qquad \quad \exists \{a_i : 0 \leq i \leq m \} \subset F\)
\\
\\
\textbf{Remember this order:}

\textbf{Exist a degree term, the polynomial has a limit, you have to define this.}

\textbf{Then you have to define the coefficient of this polynomial, even define the close interval in the same line, this is the index that can take the coefficients, \(i\)}.
\\
\\
Well I notice that I said "the remaining 6 axioms on each section", which is incorrect, there are 8 axioms cause we define the closure of addition and closure under scalar multiplication. In total are 10 axioms, not 8.
\\
Now let´s shall the \(EV1\) axiom. Commutativity of addition.

Let \(p, q in F[x]\)

P.D. \(p+q = q+ p\)
\\

Now you need to define its coefficients and degree terms of both polynomials, as well as \(k\) the maximal term degree.
\\

Thus, \(p+q = \sum_{i=0}^{k}(a_i+b_i)x^i\)
\\

\(a,b \in F\) and \(F\) is a field with commutativity of addition...

\(\forall \; 0 \leq i \leq k\) ( \(a_i + b_i = b_i + a_i\) )

Then, \(p+q = \sum_{i=0}^{k} (b_i+a_i)x^i = p+q\)
\\
\\
The following outcomes provide us the fundamental properties of a vector space


\subsection*{Theorem 1 - Cancellation law for vector addition}

If \(x, y, z\) are elements of a vector space \(V\) such that \(x+z=y+z \Rightarrow x= y\)
\\

\textbf{This theorem is useful if we want to prove that something is unique, or even to show that 2 variables are equal as we can see in the context of subspaces}
\\
\\
Suppose that \(V\) is a vector space over \(F\) and prove the following...
\begin{itemize}
    \item 1. Exist a \textbf{unique} vector \(w \in V\) such that \(x+w=x\) for each \(x \in V\)

    Additive identity
    \item 2. For each \(x \in V \) exists a unique \(y \in V\) such that \(x+y=0\)

    Additive inverse
\end{itemize}

\textbf{Solution}
\\
1. We need to see that \(w\) is a unique vector...

Let \(w_1, w_2 \in V\) such that \(x+w_1 = x\) and \(x+w_2=x\) for each \(x \in V\)

We apply the theorem 1 in this case... by the cancellation law of addition we obtain that \(w_1 = w_2\)

Thus exists a unique vector \(w \in V \) such that \(x+w=x, \forall x \in V\)

\subsection*{Theorem 1.2 - Three axioms that satisfy vector spaces, focused on vector operations}
\textbf{If \(V\) is a EV/F}, then:

1. \(\forall v \in V (0\cdot v = 0_v)\)

2. \(\forall \alpha \in F\) and \(\forall v \in V ((-\alpha)\cdot v = \alpha (-v) = -(\alpha \cdot v))\) 

3. \( \forall \alpha \in F (\alpha \cdot 0_v = 0_v)\)

\section*{Section 1.3 - Vector Subspaces}

Sub implies that there is another set that contains the subset, for this case.

Let \(V \) a EV/F

We say that \(W \subsetneq V\) is a subspace

If the operations of V, the space, are the same in W since is a subspace then, we need to verify two operations and the remaining axioms. However, it is sufficient to prove only three axioms.

\subsection*{Theorem 1.3 - Three axioms that prove a vector subspace}

Let \(V\) a EV/F and \(W \subset V\)

We will use \(\leq\) to define a subset... \(W\leq V\)

So, \(W \leq v \) if and only if:

1. \(0_v \in W\)

2. \(\forall x,y \in W ( x+ y \in W)\) - Closure of addition over the subspace

3. \(\forall \alpha \in F\) and \(\forall x \in W (\alpha \cdot x \in W)\) - Closure of multiplication over the subspace
\\
\\
To prove an 'if and only if' statement remember that you must demonstrate both the forward implication and the reverse implication, proving just one is not enough...
\\

For example, a way to prove that \(0_v \in W\) you must demonstrate that \(0_v = 0_w\) using the theorem 1, cancellation law of addition.

To prove the remaining 2 axioms, remember operations as functions...

\((+) : W^2 \rightarrow W \subset V\)
\\
\\
\textbf{Don´t forget or confuse the notation of ordered pairs}

\(W\) x \(W = W^2 = \{ (x,y)\;|\; x, y \in W\}\)
\\
\\
For this case:

\(W\) x \(W \rightarrow W \subset V\) - This indicates that the addition of any element from V (which is also in W, as the additive identity) with an element of W \textbf{results in an element of both spaces}.

This help us for the statement 1 and 2.

\((x,y) \rightarrow x+y\)
\\

\(F\) x \(W \rightarrow W\) - This is for the third statement.

\((\alpha,x) \rightarrow \alpha x\)
\\
\\
The next steps are the remaining 8 axioms that define a vector space, but this can be really easy as we show:

We want to prove the commutativity of addition

Let \(x,y \in W\) 

Since \(W \subseteq V,\;\; x,y \in V\) and V is a VS/F...

We conclude that \(x+y=y+x\)

\subsection*{Transposed Matrices}

1. Define the elements...

Let \(m,n \in \mathbb{N}\) and \(M \in F^{\textbf{mxn}} \) 
\\
\\
2. Define the method, in this case is the concept of transpose.

The transpose of \(M\) is the matrix \((M^T) \in F^{\textbf{nxm}}\) 

Notice the degree term that belongs to each matrix

\(\forall \;\textbf{i} \leq m\) and \(\forall \;\textbf{j} \leq n, \;\; ((M^T)_{\textbf{ij}} = M_{\textbf{ji}})\)
\\
\\
\textbf{If \(M^T = M\) we say that M is symmetric}
\\
\\
The collection of symmetrical matrix are a subspace, \textbf{notice that the degree term of this matrices have to be square, for instance, the size n x n}.

Let us see an example of this:

Let \(n \in \mathbb{N}\) and \(M \in F^{\textbf{nxn}} \) 

Define the concept, in this case we want to prove that transposed matrices with size square are a subspace...

\(W := \{M \in F^{\textbf{nxn}} : M^T = M\}\), then \(W \leq F^{\textbf{nxn}}\)
\\

We can prove it with \(0 \in F^{\textbf{nxn}}\)

\(\forall \;\; i,j \leq n\;\; (0_{ij} = 0) \)

Of course 0, the "normal" 0, but this 0 belongs to our subspace \(W\)!
\\

Formally:

1. Define 0 as a matrix and its tranpose

2. Define the limit of the degree term and what you want to prove, in this case:

\((0^t)_{ij} = o_{ji} = 0 \in W\)

Moreover, \(0_{ij} = 0\)

Thus (\(0^t)_{ij} = 0_{ij}\)

Summary: \(0^t = 0\)
\\

Now, prove the closure under addition and multiplication.

Notice we apply the concept of transpose to the operations, we don´t make the operations between the tranposed elements...
\\
\\
Closure under addition

\(\forall M, N \in W \;\;(M+N\in W)\), both are from the same space (still square)

Is \((M+N )^t = M+N\)

Let \(i,j \leq n\)

\((M+N)^t = (M+N)_{ji} 

= M_{ji} + N_{ji}\) by def

I want get the equality...

\(= M_{ij} + N_{ij} = (M+N)_{ij}\)

As \(M,N \in W\),

\(M_{ji} = M^t = M_{ij}\)

\textbf{We are just applying the concept that is over the subspace}

Thus, \(M_{ji} = M_{ij}\)

The same for the matrix \(N\)

Thus, \(M_{ji} + N_{ji} = M_{ij} + N_{ij} = (M+N)_{ij}\)
\\
\\
I didn´t get the following notation but make sense:

\(((M+N)^t)_{ij}\)

"We apply the transpose to the term degree \(ij\), just that... \((M+N)_{ij}\) apply the tranpose, just a comment.
\\
\\
It is the same for the scalar product.
\\
\\
\subsection*{5 subspaces applying functions}
Now we are going to present 5 examples (of different structures/dimensions) of subspaces with a function applied.
\\
\\
1. The following example is to polynomials.

Let \(n \in \mathbb{N}\{0\}\)

We define the collection \(F_n[x] := \{p \in F[x] : \alpha p \leq n\}\)

We want to achieve this statement over each operation.

Obs. In the book... \(\alpha 0 = -1\) but for us \(\alpha 0 = - \infty\)
\\

Notice that we apply a concept (I even don´t know who is but is related with \(\alpha\)), and we are going to prove that \(F_n[x]\) is a subspace.
\\

\begin{itemize}
    \item 0 is the zero from \(F[x]\) moreover, \(\alpha 0 = -\infty < n\)

Thus, \( \in F_n[x]\)

    \item Let \(p, l \in F_n[x]\)

To prove \(p+l \in F_n[x]\)

We can use the following statement without demostrate it: \(\alpha(p+l) \leq max(\alpha p, \alpha l)\)

Since \(p,l \in F[x] \rightarrow \alpha p \leq n,\;\; \alpha l \leq n \)

Then we have \(\alpha p + \alpha l = \alpha (p+l)\) which is less than or equal to n

    \item Let \(\beta \in F\) a scalar, and \(p \in F_n[x]\) 

To prove, \(\beta \cdot p \in F_n[x]\)

We know that \(\beta p \in F[x]\)

\(\alpha(\beta p) = \alpha \beta + \alpha p\)

Notice that if \(\beta = 0\) then, we get \(-\infty\) from \(\alpha \beta\), but if \(\beta\) is not equal to 0 we get a 0

Both cases are in \(F_n[x]\), the subspace
\end{itemize}

2. \(\mathbb{R}^{\mathbb{R}}\) this is a set of functions that maps from \(\mathbb{R}\) to \(\mathbb{R}\), (or is a mapping from x to y)

For this case we define \(C(\mathbb{R})\) as the collection of continuous functions

\(C(\mathbb{R}) := \{f \in \mathbb{R}^{\mathbb{R}} : f \) is continuous\}

\begin{itemize}
    \item Let \(0 : \mathbb{R} \rightarrow \mathbb{R}\)

    Any constant function is contonuous; Thus, \(0 \in \mathbb{R}^{\mathbb{R}}\)

    \item The addition of continuous functions are contonuous

    \item The same for the multiplication
\end{itemize}

\subsection*{Theorem 1.4 - V is a EV/F. If \(W_1 \leq V\) and \(W_2 \leq V\), \(then W_1 \cap W_2 \leq V\)}
This means that the intersection of \mathbf{these} subspaces also satisfy the three axioms that make it a subspace.

Let us demonstrate the theorem with the previous statement:

\begin{itemize}
    \item ¿\(0_v \in W_1 \cap W_2\)?
    
    Since \(W_1 \leq V\) and \(W_2 \leq V\)
    
    Then, \(0_v \in W_1\) and the same for \(W_2\)

    Thus, \(0 \in W_1 \cap W_2\)

    \item Let \(w_1, w_2 \in W_1 \cqp W_2\)

    To prove, the sum of these elements are in the subspace

    Since \(w_1, w_2 \in W_1 \leq W_2 \subset W_1\) and \(W_2\) (both elements in both subspaces of \(V\)).

    Then \(w_1 + w_2 \in W_1\) and \(W_2\)

    Thus, the sum is an element of the intersection.

    \item Let \(\alpha \in F \) and \(w \in W_1\cap W_2\)

    To prove, \(\alpha w \in W_1 \cap W_2\)

    Since \(w \in W_1, W_2\)...

    \(\alpha w \in W_1, W_2\)

    Thus, \(\alpha w \in W_1\cap W_2\)
\end{itemize}

\subsection*{Corollary}

If \(V\) is a EV/V, \(n \in \mathbb{N}\) and \(\forall\; i \leq n...\;(W_i \leq V )\)

\[
\bigcap_{i=1}^n W_i \leq V
\]

Is it true that if \(W_1, W_2 \leq V\), then \(W_1\cup W_2 \leq V\)?

\textbf{No}

\section*{Section 1.4 - Linear Combinations / Generating Set}

Def. \(V\) as a VS/F and \(\emptyset \neq S \leq V\)

Given \(v\in V\), we could say that \(v\) \textbf{is a linear combination if and only if:}

\(\exists n \in \mathbb{N}\) 

\(\exists \{\alpha_i : i\leq n \} \subseteq \textbf{F}\) and

\(\exists \{v_i : i\leq n\} \subseteq \textbf{S}\)

\textbf{Notice that we require the following}:

1. A natural number which serves as the limit of the summation,  from the field \mathbf{N}

2. A coefficient/scalar from the field \textbf{F} 

3. Vectors from a set, this set is the Generating Set...

\[
v = \sum_{i=1}^{n} \alpha_i v_i
\]

This means that some vectors may be expressed as linear combinations, and even a vector space as V could be composed by linear combinations.
\\
\\
Let us see an example of this concept applied on polynomials...

\(p := x^3 - 2x^2 - 5x - 3 \in \mathbb{R}[x]\)

\(q := 3x^3 - 5x^2 - 4x - 9 \in \mathbb{R}[x]\)

We define the set \(S\) with both vectors as its elements, satisfying that \(\emptyset \neq S \subseteq \mathbb{R}[x]\)

We define \(r := 2x^3 - 2x^2 + 12x - 6 \in \mathbb{R}[x]\)
\\
\\
Is the vector \(r\) a linear combination of vectors in \(S\)?

Let´s see that we can only have 2 scalars, the posibilities are:

n= 2 whose elements are \(\{ \alpha_1, \alpha_2\} \subseteq \mathbb{R}\) such that \(r=\alpha_1 p + \alpha_2 q\)
\\

n= 1 such that

\(r = \alpha_1 p\) such that \(\{\alpha_1\} \subseteq \mathbb{R}\)

\(r = \alpha_1 q\) such that \(\{\alpha_1\} \subseteq \mathbb{R}\)

Notice in both cases of n=1, one polynomial is multiplied by 0
\\
\\
These are the cases to get r but, this step is not necesary, at the end the most important part is found the values/scalars to get r, so

What is the value for \(\alpha_1\) and \(\alpha_2\)? Or just the value of \(\alpha_1\) in other case.

To found these values...

We want \(r=\alpha p + \beta q\)

Then, we need to solve the system of equations with the corresponding degree terms factorized. 

At the end we get that \(\alpha = -4\) and \(\beta = 2\)
\\
\\
If a vector is not a linear combination we need to get a contradiction, but are the same steps, solve a system of equations and see for instance that \(0=x\) with \(x\in \mathbb{R}\)\textbackslash \{0\}\

\subsection*{Span(S) - Generated Set}

\(Span(S)\) is by definition the set of all linear combinations of vetors in \(S\).

Observation.

\textbf{Notice that all we need to define a linear combination is the same as what we need for the \(Span\)...}
\\

\(w \in Span(s) \iff\)

1. \(\exists n \in \mathbb{N}\)

2. \(\exists \{ \alpha_i : i \leq n \} \subseteq F\)

3. \(\exists \{ v_i : i \leq n \} \subseteq \textbf{S}\)

\[
w = \sum_{i=1}^{n} \alpha_1 v_1
\]
\\
\\
Let us see an example

\(Span(\{(0,1,0),(1,0,0)\})\)

\(= \{\alpha(0,1,0) + \beta(1,0,0) : \alpha, \beta \in \mathbb{R}\}\)

\(= \{(\beta,\alpha,,0) : \alpha, \beta \in \mathbb{R} \}\)

\textbf{However, this element don´t generate the entire field \mathbf{R}!}

This only generate \textbf{some} elements that belong to \mathbf{R}.

If we try to color the area from a plane in three-dimensional space, we notice that only one side, which serves as its base, is shaded.

\subsection*{Theorem 1.5 - The set of lineal combinations named \(Span(S)\) is a subspace of F}

V is a VS/F and \(S \subseteq V \Rightarrow \)

1. \(Span(S) \leq V\) 

2. If \(S\;(generating-subset\:of) \subseteq W (subspace) \leq V (space) \Rightarrow Span(S)\; (generated) \subseteq W \leq V\)
\\
\\
Before proving this theorem, we need notice the following:

\begin{itemize}
    \item If \(S = \emptyset \rightarrow S \subseteq Span(S)\) such that \(Span(0) := \{0_v\}\)
    \item If \(S \neq \emptyset\) \textbf{and \(w \in S \rightarrow n=1, v_1=w, \alpha_1 = 1 \) }, \textbf{\textit{None}} is empty or zero, so we get the linear combination defined as \( \alpha_1 v_1 = 1\cdot w = w \). 
    
    Thus \(w \in Span(S)\).
    
    In addition \(S \subseteq Span(S)\) since \(w\) is the only element that composes the set S
\end{itemize}
\\
\\
Now let´s prove the first statement of the theorem...
\\

\textbf{\textit{Is \(Span(S)\) a subspace of \(V\)?}}

Let´s verify if \(Span(S)\) satisfies the 3 axioms of a subspace:
\\

There are two cases, where \(S = \emptyset\) and where \(S \neq \emptyset\) \textbf{and we need to prove in both cases the both statements from the theorem}


\begin{quote}

\begin{quote}
\(S = \emptyset\)
\\\\
1. Is \(0_{v-space} \in Span(S)\)?

\begin{itemize}
    \item With the first case we can prove that \(0_v \in Span(S)\)
    \\
    
    In addition we need to demonstrate that in fact, the addition and multiplication is contained on \(Span(S)\)

    \item Let \(x, y \in Span(S) \rightarrow x= \vec{0} =y \rightarrow x+y= \vec{0} \in Span(0)\)

    \item \(\alpha \cdot x = \alpha \cdot \vec{0} = \vec{0} \in Span(S)\)
    \end{itemize}
    
    2. Since \(W \leq V,\; \vec{0}_v \in W\) we conclude that \(Span(S) = \{\vec{0}_v\} \subseteq W\)
\\
\\
\(S \neq \emptyset\)
\\

1. Is \(0_{v-space} \in Span(S)\)?

\begin{itemize}
    \item \(0_v \in Span(S)\) for \(\exists n \in \mathbb{N},\;\exists \alpha_1 \in F\) and \(z \in Span(S)\) such that \(\alpha_1 \cdot z = 0_v = 0 \cdot z = 0_v \in Span(S)\)

    \item Let \(x,y \in Span(S)\)

    P.D \(x+y \in Span(S)\)
    \\

    \(x \in Span(S) \Rightarrow\)

    \(\exists n \in \mathbb{N}\)

    \(\exists \{\alpha_i : i \leq n\} \subseteq F\)

    \(\exists \{v_1 : i \leq n\} \subseteq S\), such that:

    \[
    \sum_{i=1}^{n} \alpha_i \cdot v_i
    \]

Now we need to define a new limit, coefficient and vector for the other element that belong to the generated set.


    \(y \in Span(S) \Rightarrow\)

    \(\exists m \in \mathbb{N}\)

    \(\exists \{\beta_i : i \leq m\} \subseteq F\)

    \(\exists \{w_1 : i \leq m\} \subseteq S\), such that:

    \[
    \sum_{i=1}^{m} \beta_i \cdot v_i
    \]

    In addition we need to be careful with the indices, we will define a new degree that \textbf{encompasses} both degrees, \textbf{from} \(\rho_1\) until \(\rho_n\) we have degree \(\alpha\) \textbf{\textit{up to}} its limit (n), and \textbf{from} \(\rho_{n+1}\) until \(\rho_{n+m}\) we have degree \(\beta\) \textbf{\textit{up to }}its limit (m)
        \[
        \rho_j =
        \begin{cases} 
        \alpha_j & \text{si } j \leq n \\
        \beta_{j-n} & \text{si } n<j\leq m+n
        \end{cases}
        \]

    And we do the same for the vectors, using the same index (j)
    \\

    \textbf{But, why we did this?}

    We are working with a sum of sums... we need to be careful with the index that serves to define the outcome from the sum of linear combinations, for at the end we are getting a new element from the \(Span(S)\)

    As summary we are \textbf{dealing} with linear combinations \textbf{which} we define as sums. This is why we introduce a new index \textbf{for the new linear combination} that belong to the span.

    \textbf{Study the index}

    \[
    x+y = \sum_{i=1}^{n} \alpha_i \cdot v_i + \sum_{i=1}^{m} \beta_1 \cdot w_i = \sum_{i=1}^{m+n} \rho_j z_j
    \]

    \item The product is easier than the previous point, we need to change nothing.
    \\
    \end{itemize}
    
    2. If \(S \subseteq W \leq V \Rightarrow Span(S) \subseteq W \)

    We want to see that the \(Spacn(S) \subseteq W\)

    Let \(x \in Span(S) \Rightarrow\) Therefore, we need to define what is necessary for a linear combination...

    \(\exists n \in \mathbb{N}\)

    \(\exists \{\alpha_i : i \leq n\} \subseteq F\)

    \(\exists \{v_i : i \leq n\} \subseteq S\) such that

    \[
    x = \sum_{i=1}^{n} \alpha_i \cdot v_i
    \]

    Notice that our vector \(\vec{v} \subseteq S \subseteq W\) such that \(\vec{v} \subseteq W\) and \(W\leq V\)

    Therefore \(x = \sum_{i=1}^{n} \alpha_i \cdot v_i = x \in W\)
    
\end{quote}

We already prove both cases; for this we concluded that \(Span(S)\leq V\)
\end{quote} 

This set of linear combinations \(Span(S)\) is a subspace
\\
\\
\textbf{Remember, when we refer to Span(S) we are talking about linear combinations as define earlier in this subsection.\\ITS ELEMENTS ARE LINEAR COMBINATIONS of a scalar and a vector} 

\subsection*{When a generating set \(S\) generates a space \(V\)}

We say that \(S\) generates \(V \iff Span(S)=V\)
\\
Notice if \(S(generating) \subseteq V(space) \Rightarrow Span(S)(generated) \leq(subspace-of) V \) in special \(Span(S) \subseteq V\)

Thus, \(S\) generates \(V \iff V \subseteq Span(S)\)

\subsubsection*{Example with Vectors}

Let define \(\vec{u} = (1,1,0), \vec{v} = (1,0,1), \vec{w}= (0,1,1)\)

Affirmation:

If \(S = \{\vec{u}, \vec{v}, \vec{w}\} \Rightarrow S\) generates \(\mathbf{R}^3\) since \(Span(S) = \mathbf{R}^3\)
\\
This means that we can obtain any vector that belongs to \(\mathbf{R}^3\) as a linear combination of 3 scalars from \(\mathbf{R}\) and the 3 vectors from S.

\textbf{To be proven/to be shown} \(\mathbb{R}^3 \subseteq Span(S)\)

That means that exits an element from \(\mathbf{R}^3\) obtained by a linear combination of 3 scalars and the 3 given vectors. 
\\

The scalars for this example need to be generalized to obtain any element of the field. 

\(\exists x \in \mathbb{R}^3\)

With this, \(\exists \alpha, \beta, \rho \in \mathbf{R}\) such that:

\(x = \alpha \cdot \vec{v} + \beta \cdot \vec{v} + \rho \cdot \vec{w}\) 

The entries of x are \(x_1, x_2, x_3\) need be equal to the entries of the linear combination...

\((x_1, x_2, x_3) = (\alpha + \beta, \alpha + \rho, \beta + \rho)\)
\\

Now we need to solve a system of equations, this could be the anti-dummies method (tidy stairs).

\textit{We need to obtain only one solution from the system, only one is enough, if you obtain it, that means indirectly, you could obtain any element from the space since we generalized the solutions.}

Finally, we can see that, in fact, \(\exists \alpha, \beta, \rho \in \mathbf{R}\) with one solution such that \(x \in Span(S)\)

\subsubsection*{Example with Polynomials}
Let´s see another example with polynomials

\(p = x^2 + 3x -2, q = 2x^2 + 5x -3\) and \(r=x^2 - 4x +4\)

Notice, \(S = \{ p,q,r\} \subseteq \mathbb{R}_2[x]\), the collection of polynomials with a \textbf{term degree less than or equal to 2}

Affirmation

\(S\) generaate \(\mathbb{R}_2[x]\)

Let x be an element that belongs to the vector space... \(x \in \mathbf{R}_2[x]\)

\textbf{That means a limit exists} but in this case is already defined by the field...

\textbf{There exist} 3 scalars from our field \(\mathbb{R}\) that we will define as \(\alpha, \beta, \rho\)
\\

\(x=ax^2+bx+c\)

There is no \textbf{\textit{guarantee}} that a, b and c are equal to 0

Let´s see if we have a solution in our system of equations but to get this, we need to remember that the system is formed by a linear combination (with the elements we define) which is:

\(\alpha \cdot p + \beta \cdot q + \rho \cdot r = ax^2 + bx +c\)

\(= (\alpha+2\beta-\rho)x^2 + (3\alpha + 5\beta -4)x + (-2\alpha -3\beta +4\rho))\) such that we \textbf{form/create} an equallity with the values of the polynomial \(x\).
\\

Solving the system, we find a solution, that is enough.
\\

Thus, \(\{p,q,r\}\) generate the space \(\mathbf{R}_2[x]\)

\subsubsection*{Example with Matrices}

Let \(L =
\begin{bmatrix}
0 & 1 \\
1 & 1 \\
\end{bmatrix}, M =
\begin{bmatrix}
1 & 0 \\
1 & 1 \\
\end{bmatrix}, N =
\begin{bmatrix}
1 & 1 \\
0 & 1 \\
\end{bmatrix}, Ñ = 
\begin{bmatrix}
    1 & 1 \\
    1 & 0 \\
\end{bmatrix}
\)
\\

Then, \(S = \{L,M,N,Ñ\} \subseteq \mathbb{R}^{2x2}\)

To be proven the space \(\mathbb{R}^{2x2}\) is generated by S... that means prove that \(\mathbf{R}^{2x2} \subseteq Span(S)\), \textbf{the vector space is subset of the generated set}.
\\

Let \(A \in \mathbb{R}^{2x2}\)
\\

P.D. \(\exists \{\alpha, \beta, \rho, \sigma\} \in \mathbf{R}\) such that the element A is a linear combination of these elements and the matrices defined.

\(A = \begin{bmatrix}
    A_{1,1} & A_{1,2} \\
    A_{2,1} & A_{2,2} \\
\end{bmatrix} = \begin{bmatrix}
    a & b \\
    c & d \\
\end{bmatrix} \Rightarrow \alpha L + \beta M + \rho N + \sigma Ñ = \begin{bmatrix}
    \beta + \rho + \sigma & \alpha + \rho + \sigma \\
    \alpha + \beta + \sigma & \alpha + \beta + \rho \\
\end{bmatrix}\)
\\

Solving the system of equations we obtain a solution, with this we conclude that \(S\) generate the space \(\mathbb{R}^{2x2}\)

\subsubsection*{Example of something that fails to/does not generate}
Remember that an augmented matrix is "A \textbf{matrix that includes both the coefficients of the variables and the constants from a system of linear equations}."
\\\\
Let \(L = \begin{bmatrix}
    1 & 0 \\
    0 & 1 \\
\end{bmatrix}, M = \begin{bmatrix}
    1 & 1 \\
    0 & 1 \\
\end{bmatrix}, N = \begin{bmatrix}
    1 & 0 \\
    1 & 1 \\
\end{bmatrix}\)
\\

\(S = \{L, M ,N\} \subseteq \mathbb{R}^{2x2}\)

Affirmation 

S does not generate \(\mathbb{R}^{2x2}\)

If I propose scalars as \(\alpha, \beta, \rho \in \mathbb{R}^{2x2}\) we can notice that the matrix \(\begin{bmatrix}
    0&0\\
    0&1\\
\end{bmatrix}\) is an element from the space that \textbf{cannot be obtained }as a linear combination \textbf{of the defined vectors and scalars.}

By solving the system of equations using an augmented matrix we obtain a row with an inconsistency, 1 = 0

This means that there is no solution in the system and as a result, S does not \textbf{span} the space, so \(\mathbb{R}^{2x2} \nsubseteq Span(S)\)
\\

\(\therefore \neg \exists \{\alpha, \beta, \rho\} \subseteq \mathbb{R} : A = \alpha \cdot L + \beta \cdot M + \rho \cdot N \)

\subsection*{Trace of a Matrix - Introduction of this function}
We already define the transposed matrix...

Let \(m, n \in \mathbb{N}\) and \(A \in F^{mxn}, \forall i \leq m,\; \forall j \leq n\)

If we have a matrix \(A = (a_{\textbf{ij}})\), then \(A^t = (a_{\textbf{ji}})\)
\\\\
\textbf{The trace of a matrix is the sum of its diagonal entries...}

tr(M) = \(\sum M_{ii}\)

tr(N) = \(\sum N_{ii}\)

\section*{Section 1.5 - Linearly Dependent and Independent}
\subsection*{Is a set (as the generating set) linearly dependent?}
Def. \(V\) is a VS/F and \(S \subseteq V\), the generating set is a subset of the space

\textbf{We say that the generating set is linearly dependent if}

\(\exists n \in \mathbb{N}\) - a limit/term degree

\(\exists \{ v_i : i \leq n\} \subseteq S\)

\(\exists \{ \alpha_i : i \leq n\} \subseteq F\)

We define the same elements from a linear combination... a limit, vectors, and scalars. Such that:

\[
\vec{0} = \sum_{i=1}^{n} \alpha_1 v_1 
\]

But we need to establish conditions because we can easily obtain \(\vec{0}\) doing the scalars equal to 0
\\
\begin{itemize}
    \item Condition 1 : \(i \neq j \rightarrow v_i \neq v_j\) 

This means that the \textbf{vectors are different from each other.} 

Called \textbf{"Faithfully indexed without repetitions"}
\\

    \item Conditions 2: The scalars need to be different from 0. 
\end{itemize}

With these conditions, we rewrite the statements as follows:

\(\exists n \in \mathbb{N}

\exists \{v_i : i \leq n \} \subseteq S\) without repetitions

\(\exists \{\alpha_i : i \le qn \} \subseteq F \text{\textbackslash}
 \{0\}\)

Such that:

\[
\sum_{i=1}^{n} \alpha_1 v_1 = \vec{0}
\]

%%If the result of the linear combination is unequal to 0 we call this linearly independent.
\\

\textbf{\textit{Let´s see an example with vectors...}}

Let \(w_1 = (1,3,-4,2), w_2 = (2,2,-4,0), w_3=(1,-3,2,-4)\) and \(w_4=(-1,0,1,0)\)

\(S=\{W_i : i \leq 4\} \subseteq \mathbb{R}^4\)

\textbf{Affirmation - S is linearly dependent, which means that its solutions as a linear combination satisfy to be unequal to 0.}
\\

Then , \(\exists \alpha, \beta, \rho, \sigma \in \mathbb{R}\text{\textbackslash\{0\}}\) such that:

\(\alpha \cdot w_1 + \beta \cdot w_2 + \rho \cdot w_3 + \sigma \cdot w_4 = \vec{0}\)
\\

Now we can solve this as a system of equations.

One solution to this system is as follows: 

\(\alpha = 4, \beta = -3, \rho = 2, \sigma = 0\)

There is not problem if one scalar is equal to 0, \textbf{we only require that all other scalars must not be 0.}
\\

\textbf{\textit{Now let´s see an example with matrices...}}

Let \(M \begin{bmatrix}
    1&-3&2\\
    -4&0&5\\
\end{bmatrix}, N = \begin{bmatrix}
    -3&7&4\\
    6&-2&-7\\
\end{bmatrix}, Ñ = \begin{bmatrix}
    -2&3&11\\
    -1&-3&2\\
\end{bmatrix}\)
\\

\(S = \{M,N,Ñ\} \subseteq \mathbb{R}^{2x3}\)

Affirmation - S is LD, this means that its linear combination is equal to the vector 0.

\(\exists \alpha, \beta, \rho \in \mathbb{R} \text{\textbackslash}\{0\} : \alpha \cdot M + \beta \cdot N + \rho \cdot Ñ = \vec{0}?\)
\\

A solution to this system is as follows:

\(\alpha = 5, \beta = 3, \rho =-2\), which are unequal to 0

\textbf{Remember that it is enough for only one value to be unequal to 0.}

\(\therefore S\) is LD.

\subsection*{Observations about linearly independent sets}
\begin{itemize}
    \item The empty set is \textbf{linearly independent} because the set could have only trivial solutions. 
    \item In any vector space, a set with only one element non-null is \textbf{linearly independent.} This makes sense because the only scalar which could obtain a 0 vector is the product between the unique vector and the scalar 0.
    \item A generating set \(S\) is linearly independent iff the unique solution to the linear combination is the trivial solution, where all scalars are equal to 0.
\end{itemize}

\subsection*{Theorem 1.6 - Linearly Dependent Sets, related by subset inclusion}

Let \(V\) be a vector space, and let \(S_1 \subseteq S_2 \subseteq V\)

If \(S_1\) is linearly dependent, which is a subset of another set \(S_2\), then \(S_2\) is also linearly dependent.

\section*{Section 1.6 - Basis}
Def \(V\) is a vector space under F and the set \(B \subseteq V\)

\textbf{B is a base for V if and only if:}
\begin{itemize}
    \item B is linearly independent
    \item V = Span(B)
\end{itemize}

Let´s see some examples...

\subsection*{Trivial example:}

If \(V\) is trivial such that (\(V = \{ \vec{0}\}\))

\(\emptyset\) is a base for V

\begin{itemize}
    \item The empty set is linearly independent 
    \item \(Span(\emptyset) = \{\vec{0}\} = V\)
\end{itemize}

\subsection*{Example with standard vectors}

F is a field and \(n \in \mathbb{N}\)

\(\forall i \leq n\) we define \(e_i \in F^n\) - Index and limit of our vectors

If n = 3...

\(B = \{e_i : i \leq n\} \subseteq F^n\)

\begin{itemize}
    \item B is linearly independent, we can demonstrate it solving a system of equations obtained from its linear combination but generalized.
    \item To be proven that B generates the space V

    \(\exists \{x_i : i \leq n\} \subseteq \mathbb{F}\) 

    \(x = x_1 (1,0,...,0) + x_2 (0,1,...,0) + ... + x_n  (0,0,...,1)\)
    
    \(= x_1 \cdot e_1 + x_2 \cdot e_2 + ... + x_n \cdot e_n = \)
    
    \[
    \sum_{i=1}^{n} x_i \cdot e_i \in Span(S) = \mathbb{F}^n
    \]

With that sum we can obtain any element of the space... 

\textbf{\textit{We call this set B the standard basis.}}

Notice that there are no limitations, it is not necessary to define the \textbf{scalars as different from each other} or unequal to 0.
\end{itemize}

\subsection*{!!! Example with matrices}

Let \(m,n \in \mathbb{N}, \forall k \leq m\) and \(\forall l \leq n\) 

\textbf{\textit{kl defines the number of standard matrices required for the space with size \(kl\)}}

\textbf{\textit{nm defines the size of the vector space}}
\\

We define \(E^{kl} \in F^{mxn}\)

\(\forall i \leq m\) and \(\forall j \leq n\)...

\((E^{kl})_{ij} = \begin{cases}
    1, i = k \text{ and } j =l\\
    0, i \neq k \text{ or } j \neq l
\end{cases}\)
\\

For example in the space \(\mathbb{R}^{3x2}\) with \(k = 1,2,3\) and \(l=1,2\)

\textbf{The number of standard matrices are 6 for this example}... \(E^{11}, E^{12}, E^{21}, E^{22}, E^{31}, E^{32}\)
\\

Now

\(B := \{E^{kl} : k\leq m \text{ and } l \leq n\} \subseteq F^{mxn}\)
\begin{itemize}
    \item We need to see that \(B\) is linearly independent.
    \item The space is equal to the Span(B)

    We take a fixed element of the space. We already have the limits/degree for our operations.

    Let \(A \in F^{mxn}\)

    We know this matrix has to be written as a linear combination...

    Let´s define some scalars... \(\exists A_{ij} \in F\)

    \(A = A_{11} (E^{11}) + A_{12} (E^{12}) + A_{21} (E^{21}) + ... + A_{kl} (E^{kl})\)

    \[A = (\sum_{j=1}^{2} A_{1j} E^{1j}) + (\sum_{j=1}^{2} A_{2j} E^{2j})\]

    One sum serves to indicate the row and the other sum serves to indicate the column

    \[
    A  = \sum_{i=1}^{m} \sum_{j=1}^{n} A_{ij}E^{ij}
    \]
\end{itemize}

\subsection*{Examples with polynomials}

Let \(n \in \mathbb{N}\) and \(B := \{x^i : i \leq n\} \subseteq F_n[x]\)

The base is a subset from the vector space
\begin{itemize}
    \item Demonstrate that B is linearly independent, remember that for this type of exercises we usually solve a system of equation however, when we generalize a basis as we do in this case it is more intuitive without compute the scalar values.
    \item B generates the space \(F_n[x]\)? Which means that \(F_n[x] = Span(B)\)

    Let´s define an arbitrary element from the space 

    \(p \in F_n[x]\) such that \(p \in Span(B)\)

For this, \(p = a + bx^1 + cx^2 + ... + z x^n\) could be written as a linear combination\textbf{ that belongs to span(S)}, we need define scalars of course. 

    We already have a term degree so it is not necessary define one

    \(\exists \{ \alpha_i : i \leq n\} \subseteq F\)

    Such that:

    \[
    p = \sum_{i=1}^{n} \alpha_1 x_i  \in Span(B)
    \]
\end{itemize}
\\
\\
Let´s examine another example but now proving that a basis is linearly independent...
\\
Basis, how is the set?

If \(B := \{x^i : i \in \mathbb{N}\cup \{0\}\} \subseteq F[x]\)

Is \(B\) linearly independent?

The value of \(n\) does not matter for this case, as this result \(n\) must hold for any value...

We need a set of vectors \textbf{faithfully indexed withoud repetitions} and of course, a corresponding set of scalars...
\\

The elements from the basis, how are?

\(\forall n \in \mathbb{N}\) and \(\forall \{p_i : i \leq n \} \subseteq B\)
\\

The basis B is linearly independent if and only if there exists a set of scalars (\(\alpha_i : i \leq n\)) such that:

\[
\sum_{i=1}^{n} \alpha_i p_i = \vec{0} \Rightarrow \forall i \leq n (\alpha_i = 0)
\]

Remember that the scalars need to be equal to 0
\\

This is the key: For the elements that belongs to the basis \(B\) it is necessary define their degree that intuitively the degree defined must be less than or equal to \(n\)...
\\
\\
\(\forall i \leq n (p_i \in B)\) - The elements that belongs to the basis 

\(\Rightarrow \exists j \in \mathbb{N}\cup\{0\}\) such that \(p_i = x^j\) - The degree for each element of the basis.



\subsection*{Theorem 1.8 - Basis, there exists an unique scalar set }
Define of basis...

Let \(V\) an VS/F

Let \(B = \{v_i : i \leq n\}\) (faith), is a basis for B

Then, \(\forall x \in V,\;\exists\textbf{!}\{\alpha_i : i \leq n\} \subseteq F\) such that:

\[x = \sum_{i=1}^{n} \alpha_i v_i\]
\\
\\
Let´s demonstrate the theorem:
\\

How can we demonstrate uniqueness?

Suppose, there exists another element that satisfies the same and it has to led that are equal:

By definition of basis, \(\exists \{\alpha_i : i \leq n\} \subseteq F\) - We know that this scalar set satisfies.

Now we need another scalar set that combinated with the vectors of the basis as a linear combination we obtain any element of the vector space.

Let \(\{\beta_i : i \leq n \} \subseteq F\) faithfully indexed, such that:

\[x = \sum_{i=1}^{n} \beta_i v_i\]

Since \(\vec{0} = x-x = \sum_{i=1}^{n} (\alpha_i - \beta_i) v_i\) which means that \(\{\alpha_i - \beeta_i : i\leq n\} \subseteq F\)

\(\forall i \leq n (\alpha_i - \beta_i = 0) \Rightarrow \forall i \leq n  (\alpha_i = \beta_i)\)

\subsection*{Theorem 1.9 - Cardinality. There exists a basis contained in a set S which generates the vector space, how find that elements?}
\(V \) is a VS/F generated by a FINIT set \(S\) such that \(S \subseteq V\)

Then, \(\exists B\subseteq S\) (B is a basis that is contained into the set S and it is a basis for the vector space)

This means, as we see, that \(Span(B) = V\)

\subsubsection{The meaning of the idea}
A concept not to forget is that the elements we will \textbf{discard} are in S, we are going to check if these are L.D. If they do, they are candidates for basis.

\textbf{We want to get a basis for the space, we are going to looking for a set.}

To be proven:

|\(|X|\)| - Cardinality

\(|S| \in \mathbb{N}\cup \{0\}\)
\\
\\
Case 1. \(|S| = 0 \rightarrow S = \emptyset\)

Means that \(V ? \{\vec{0}\}\) and as we see, \(\emptyset\) is a basis for V

Therefore if we take \(B = \emptyset\), which is the unique set that generates \(Span(B) = \emptyset\)

Then, \(B \subseteq S\) and \(B\) is a base for V
\\
\\
Case 2. THE SET, \(S \neq \emptyset\)

Case 2.1 : \(S = \{\emptyset\}\)

\(V = Span(B)\) by definition that we already see, \textbf{and both are equal to 0}

And how \(\emptyset \subseteq S\) we take \(B := \emptyset\) that satisfies get \(Span(S) = 0\)
\\

Case 2.2 : \(S \neq \{0\}\)

Then an element exist into the set, \(\exists v_1 \in S \text{\textbackslash} \{0\}?\)

In other words, is the vector \(\{v_1\}\) linearly independent?

This means that the linear combination of this vector with a scalar from the set \(\alpha_i\) we obtain \(\vec{0}\), but there were some points to satisfy as \textbf{different vectors between each other} and the scalars \textbf{have to be 0}

But the answer of the teacher is that the vector is L.I, because it is different from 0... ¿?
\\

\(\exists v_2 \in S\text{\textbackslash}\{v_1\} : \{v_1, v_2\}\) are Linearly Independent?

There are two answers, yes or no...
\begin{itemize}

    \item No, it is not L.I...

    \textbf{In this case is easier because we only propose the set \(\{v_1\}\) as our base.}

    \(B := \{v_1\}\)
    
    \item Yes, it is linearly independent...

    We fixed \(v_2\in S\text{\textbackslash} \{v_1\}\)

    \(\exists v_3 \in S\text{\textbackslash} \{v_1, v_2\} : \{v_1, v_2, v_3\}\) are linearly independent?

    Yes or not.
    
    We fixed now \(v_3, \exists v_4?\)
    
    Yes or not

    ...
\end{itemize}

\textbf{If we continue with the answer of yes, this breaks the hipotesis of be FINITE, there is not moment to stop.}

So we need to establish a limit at \(n\)...

\(S\) is FINITE which means that there \(\exists n \in \mathbb{N}\), a set of FINITE vectors limited by \(n\)...

Here, is the key:

\(\forall w\in S\text{\textbackslash} \{v_i : i \leq n\}\) - \textbf{This is the finite basis}

\subsubsection*{Is a basis? What you need to prove}
If this new basis it is, then this set satisfies:
\\

1. Is \(B\) linearly independent

B in this case is L.D. by construction, as we do in previous steps.
\\

2. \(Span(B) = V\), the generating set could be any set contained into the vector space.

We need to take an arbitrary element from the space and make a linear combination with the previous vectors and scalars defined.

But maybe this is not the correct way to do this.





\subsubsection*{Example of theorem 1.9}
Let \(v_1 = (2,.3,5),\; v_2=(8,-12,20),\; v_3=(1,0,-2),\; v_4=(0,2,-1),\; v_5=(7,2,0)\)

\(S:=\{v_i : i \leq 5\}\subseteq \mathbb{R}^3\)

Affirmation : \(\mathbb{R}^3 = Span(S)\)

\textbf{We want to find a basis}, this is the function of the previous theorem.

Find a basis such that \(B \subseteq S\) for \(\mathbb{R}^3\)
\\
\\
1. Fix an element \(w_1 \in S\text{\textbackslash}\{0\}\) we delete 0 because it is a necessary requirement, if we don´t delete it, it is trivial, which we don´t want.
\\
\\
2. The next step is to ask ourselves if this vector is linearly independent

Of course, it is linearly independent, think about it, we only have a vector such that into a linear combination with a scalar equal to 0 we obtain the vector \(\vec{0}\). That makes sense.

Respecting to the value of \(w_1\) there is not problem with what of the 5 possible vectors that we gave, it takes.

Notice that there are only 4 possible values for \(w_2\)
\\

\(\exists w_2 \in S\text{\textbackslash} \{w_1\} : \{w_1, w_2\}\) are linearly independent?

\textbf{Remember, we fixed an element... } this is arbitrary, for example \(w_1 = v_1\)

\(v_1\) is our fixed element, \textbf{\(w_2\) is also arbitrary}, for example \(w_2 = v_2\)

Notice that \((-4)\cdot v_1 + 1 \cdot v_2 = \vec{0}\)

A scalar is non equal to 0, that means that is linearly dependent, therefore is not an element for the basis that we are trying to get.
\\

\(\{w_1, v_3\}\) is linearly independent?

\subsection*{Trick to check if a set is linearly independent}

A set is linearly dependent \(\iff \exists \alpha \in F (x = \alpha \cdot y,\;y = \alpha \cdot x)\)
\\
\\
Let´s see that in this case...

\(w_1 = (2,-3,5) = \alpha \cdot (1,0,-2) = (\alpha, 0 , -2 \alpha)\) but in this case there is not a scalar such that multiplying \(\alpha\) times 0 we could get -3

\(\therefore \neg \exists \alpha \in \mathbb{R} (v_3 = \alpha \cdot w_1) \Rightarrow\) are linearly independent.

We already have a second element for the basis.

\(w_2 = v_3\)
\\

\(\exists w_3 \in S\text{\textbackslash}\{w_i : i \leq 2\}\) such that it is linearly independent?

In other words, Is the set \(\{w_1, w_2, v_4\}\) linearly independent?

In this case we are going to prove it with a system of equations obtained by a linear combination.

\(\exists \alpha, \beta, \rho \in \mathbb{F} (\alpha w_1 + \beta w_2 + \rho v_4 = \vec{0})\)

Solving it, we get that \(\alpha = \beta = \rho = 0\)

\(\therefore \{w_1, w_2, w_3 = v_4\}\) is linearly independent.

At last, we need to see if the set \(\{w_1, w_2, w_3, w_4 = v_5\}\) is linearly independent

In this case, solving by a system of equations, we obtain that \(\alpha = 2, \beta = 3, \rho = 4, \sigma = -1\) being different of 0.

Therefore, this set is not linearly independent...
\\
\\
Our basis for this vector space is the set \(\{w_1 = v_1, w_2 = v_2, w_3 = v_4\}\)

\subsection*{Theorem 1.10 - The dimension of a space is equal to its cardinality}
Let \(V\) an VS/F which has an infinite basis then \(dim(V)\) is equal to the cardinality of any basis for V

\(dim(\mathbb{R}^3) = 3\)

\subsection*{Corollary - If the vector space has a basis with size \(n\) then ALL the basis for the space has precisely \(n\) elements}

\subsubsection*{Examples of the previous theorem - Vectors, matrices, polynomials, field \(\mathbb{C}\)}
\(dim(\{\vec{0}\}) = 0\) - In this case, \(\vec{0}\) is the unique vector space with dimension 0 then, \(\vec{0}\) is the unique basis for the vector space, since is a basis is also a generating set and linearly independent.

\(V = Span(\vec{0}) = \{\vec{0} = B\}\)
\\
\\
\(dim(F^n) = n\)

\(|\{ e_i : i \leq n \}|\) = \(n\) - This set of standard vectors is a basis for the vector space 

\textbf{|| The cardinality represents the number of elements without repetitions}
\\
\\
\(dim(F^{mxn}) = mxn\)

\(|\{ E^{kl} : k \leq m, l \leq n\}| = mxn\)
\\
\\
\textbf{The standard vectors and matrices are the easiest basis that we can find, it is trivial.}
\\
\\
\(dim(F_n[x])\)

Let´s define a basis \(B := \{ x^i : 0 \leq i \leq n \}\) for this vector space.

\(|B| = n+1\)

\(\therefore dim(F_n[x]) = n+1\) - From 0 to n 
\\
\\
\(dim(\mathbb{C})\)

There are two cases for this field...

\begin{itemize}
    \item \(dim(\mathbb{C}) = 1\)

    \(B := \{1\}\) is a base with dimension 1 for \(\mathbb{C}\)

    This means that that \(Span(B) = \mathbb{C}\), we can see that \(v \in \mathbb{C} \rightarrow v = v\cdot 1,\; i \in \mathbb{C} \rightarrow i = i\cdot 1\)

    \textbf{BUT it is not linearly independent, it is dependent}

    Let \(\alpha \in \mathbb{C}\) that belongs to the vector space under \(\mathbb{C}\)

    \(\alpha := i \in \mathbb{C}\text{\textbackslash} \{0\}\) and this does not satisfies the definition of linearly independent.
    \item \(dim(\mathbb{C}) = 2\)

    \(B := \{1, i\} \subseteq \mathbb{C}\)

    If \(z \in \mathbb{C} \Rightarrow \exists \alpha, \beta \in \mathbb{B} (z = \alpha + \beta i)\)

    \(\therefore z = \alpha + \beta i \) and \(z \in Span(S)\)

    But, is linearly independent? Yes

    Let \( \alpha + \beta i = 0 = 0 + 0i\) such that \(\alpha = 0, \beta i = 0i\) which is linearly independent.
\end{itemize}
\\
\(\therefore Dim_\mathbb{R} (\mathbb{C}) = 2 = |B|\) which is linearly independent.
\\
\(\therefore Dim_\mathbb{C} (\mathbb{C}) = 1 = |B|\) which is linearly dependent.
\subsubsection*{Applied Examples}

1. Let \(p = x^2 + 3x - 2\), \(q = 2x^2 + 5x - 3\) and \(r = -x^2 -4x-4\)

The first thing we need to check is that \(p \neq r+q, q \neq p+r, r \neq p+q\), for this case \textbf{none are fullfiled, none of them is met, none holds true.}

The basis \(B = \{ p,q,r \}\) generates the vector space \(\mathbb{R}_2[x]\)

Notice that \(dim(\mathbb{R}_2[x]) = 2+1 = 3\) which satisfies the previous theorem, was commented on, and \(|B| = 3\)

\subsection*{Corollary/Trick to check that a vector space with dimension \(n\) and a set with cardinality \(n\) are linearly independent}

That set we are attempting to find is \(B\) which has a cardinality of \(n\). Since \textbf{the vector space has a dimension of }\(n\), this is sufficient to affirm that the set is linearly independent. Additionally, we can apply the trick of checking whether a vector in the set can be expressed as a linear combination of the others.

\(\therefore B\) is linearly independent and is a basis for the vector space.
\\
\\
2. Let \(\begin{bmatrix}
    1 & 1\\
    1 & 0\\
\end{bmatrix}, M = \begin{bmatrix}
    1 & 1\\
    1 & 0\\
\end{bmatrix}, N = \begin{bmatrix}
    0 & 1\\
    1 & 1\\
\end{bmatrix}, Ñ = \begin{bmatrix}
    1 & 0\\
    1 & 1\\
\end{bmatrix}\) 
and \(S = \{ L,M,N,Ñ \} \subseteq \mathbb{R}^{2\text{x}2}\)
\\

Is this set \(S\) a basis?

We need to check the proposed basis satisfies the following:
\\

1. \(Span(S) = \mathbb{R}^{2 \text{x} 2}\)

Which, in other sense, the elements of the basis might generates all the elements of the vector space.
\\

2. Is linearly independent

We can check this with the previous trick, let´s see that \(|S| = 4\) and \(\mathbb{R}^{2x2} = 4\)

\(\therefore S\) is a basis for the vector space.

\subsection*{Theorem 1.11 - Basis of a vector space with finite dimension and the relation with subspaces}
If \(V\) is a VS/F with \(dim<\infty\) (finite dimension) then there exists a basis B for V such that B is a finite set and \(W\leq V \Rightarrow dim (W) \leq dim(V)\), we obtain V=W
\\

We have 2 bases, one for the space and another for W, the subspace.

\subsection{Corollary - Let \(L \subseteq V\)} such that is linearly independent then, \(\exists H \subseteq V (L \cup H )\) is a BASIS for V

To be proven the theorem:
\begin{itemize}
    \item Case 1 : \(W = \{\vec{0}\}\)

    \(dim(W) = 0 = dim(V)\)

    \(\therefore\)\textbf{ the basis for this subspace that is equal to the space is 0}

    \item Case 2 : \(W \neq \{\vec{0}\}... \; W \nsubseteq \{\vec{0}\} \land \{\vec{0}\} \nsubseteq W \text{ and } \therefore W \nsubseteq \{\vec{0}\}...?\)

    Since the space is non equal to the zero vector...

    \(\exists w_1 \in W \text{ since }W \neq \vec{0}),\text{ therefore, } W = \{w_1\}\) \textbf{which is linearly independent even. We already have an element for our basis.}
    \\
    
    \(\exists w_2 \in W \text{\textbackslash}w_1\) such that \(\{w_1, w_2\}\) is linearly independent?

    - Yes, we add this element into de set

    - No, in any moment we need to stop.

    In this case, if the answer is Yes, then we already have 2 elements, behold:

    \(\exists H \subseteq V : H \cup \{w_!, w_2\}\) are linearly independent.

    Notice: 
    \begin{itemize}
    \item This new set has \textbf{cardinality H+2}

    \item \textbf{W will be the total, the complete union after adding H to the previous set with 2 elements.}
    \end{itemize}

    After this, we can continue working with the other set that we were \textbf{merging}, and wonder if \(\exists w_3?\)

    But at this point, there must exists a limit, remember that the space and therefore the subspace, have a finite basis.

    We can propose \(m\) as the limit, but \textbf{how do we know that it ends there}? Let´s define the following:

    Let \(m = dim(V)\) and \(\{w_i : i \leq m\}\) such that \(n \leq m\)

    Notice this important point: \(n \leq m\), that means we need to find a limit between the two.
   
    But having \(m\) as the limit that cannot be surpass by the elements, if I propose the existence of \(w_{m+1}\), it may not be possible.
    \\

    W is a finite set but now \textbf{the process is bounded}(acotado), it cannot \textbf{surpass}(sobrepasar) the limit \(m\) which is the maximum number of elements that can form the basis.
    \\
    
    Then, \(\exists k \in \mathbb{N} (\{w_i : i \leq k\} \subseteq W)\) \textbf{faithfully indexed}, linearly independent and

    \(\forall v \in W\text{\textbackslash}\{w_i : i \leq k | \{w_i : i\leq k\}\cup \{v\}\}\) is linearly independent.

    \(\therefore k \leq m\)

    Then, \(B:=\{w_i : \leq k\}\) is linearly independent.
    \\
    
    Now it remains to be proved \(W = Span(B)\)

    We need to convince us that \(W \subseteq Span(B) \land Span(B) \subseteq W\)

    Remember that \(B \subseteq W\) then, if \( \in W\text{\textbackslash} B \Rightarrow B \cup \{v\}\) is linearly dependent \textbf{merged them.}

    Explication:
    
    This makes sense, the basis have to be linearly independent to can get any element from the space or subspace, we are selective with the elements that belongs to the basis then, if we add an element that can be obtained with the elements from the basis (means that it belongs to the generated set)... this new set is linearly dependent because the element added (for this case \(v\)) can be obtained from the multiplication of a scalar with another vector of the basis, this is satisfied and therefore, is linearly dependent.

    The union of a set (that is a subset of the space) with an element that belongs to the space but not into its basis then are linearly dependent.
    \\
    
    \(dim(W) = k \leq m = dim(V)\)

    But, remember that we said that \(dim(V) = dim(W) = m = k\)

    Then, \(\exist \{v_i : i \leq m \} \subseteq V\) which is a basis for V and is linearly independent, since \(|L| = m = dim(V)\) and by the corollary which says that if we have a cardinality equal to the dimension of the space then we have that are linearly independent, additionally we already see that is the Span equal to the space, we obtain that is a basis

    \(\therefore V = Span(S) = W\)

    Interpretation: Getting that V is equal to the span means that the generating set S for this case is a basis for V, adding that needs to be linearly independent of course.
\end{itemize}
\subsection*{Reminder - \(S \cup \{v\}\) is linearly dependent \(\iff v \in Span(B)\) taking \(v\) from \(W\text{\textbackslash}B\)}

\subsection*{Examples, Is a set a basis for a subspace?}

\subsubsection*{A space with dimension m could be a subspace for a space with dimension n such that \(m \leq n\)}
1. \textbf{Let´s define the subspace as follows:} 

\(W := \{x \in \mathbb{F}^5 : x_1 + x_3 + x_5 = 0\}\) and \(x_2 = x_4\), \textbf{this have to be satisfied by the elements that we will propose for the basis of W.}

Then, \(W \leq \mathbb{F}^5\)

Is the set \(B := \{(-1,0,0,0,1),(-1,0,1,0,0),(0,1,0,1,0)\} \subseteq W\) a basis? 

Behold, each vector has to satisfy the definition of W, this satisfies.

We have 2 ways to prove that this set is linearly independent, solving a system of equations \textbf{or using a simpler approach}(enfoque más simple/método más sencillo)-checking that the vectors are not scalar multiples each other.

Notice that we can obtain any element of the vector space through a linear combination with the vectors of the basis with a scalar set.

The most important part is that the dimension of this subspace is 3, not 5, which could be a mistake \textbf{on out part}.

\(\therefore B\) is a basis for \(W \Rightarrow dim(W) = 5\)
\\
\\
2. Let´s define \(W\) as the subspace that consist of all diagonal matrices.

Let \(n\) such that it is the dimension of our matrices, remember that diagonal matrices are exclusive from square matrices.

\(B :=\{E^{ii} : i \leq n\} \subseteq W\)

\(B \subseteq \{E^{kl} : k,l \leq n\}\)

\(\therefore B\) is linearly independent and \(W =Span(B)\)

Notice that, the dimension of \(W\) is \(n\), it makes sense because the size of a matrix that belongs to the space is the same as its diagonal, because all of this serves only in square matrix.
\\
\\
\subsubsection*{Example with transposed matrices, be careful with the size, interpret it correctly}
3. 


\section*{1.7 Linear Transformations}
Now we want to connect vector spaces, for that we use the transformational function.

Definition. 

Let \(V, W\) be vector spaces.

\(T:V\rightarrow W\) is linear if \(\forall x,y \in V,\;\forall\alpha \in F\)...

1. \(T(x+y) = T(x) + T(y)\)

2. \(T(\alpha\cdot x) = \alpha \cdot T(x)\)

For any element of the domain.
\\
\\
To be proven in a summary way:

\(T(\alpha \cdot x + y) = \alpha \cdot T(x) + T(y)\)

\(\rightarrow \alpha \cdot x \in V,\; \therefore \alpha x, y \in V\)

1. \(\rightarrow T(\alpha \cdot x + y) = T(\alpha \cdot x) + T(y)\)

2. \(\rightarrow T(\alpha \cdot x) = \alpha \cdot T(x)\)

\(\therefore T(\alpha \cdot x + y) = T(\alpha \cdot x) + T(y) = \alpha \cdot T(x) + T(y)\)

Now let us see some examples

\subsection{Example 1}

Let \(T : \rightarrow R^{2} \rightarrow R^{2}\) and \(\forall x \in \mathbb{R}^{2}\;(T(x) = (2x_1 + x_2 ,\:x_1))\) - This is transformed, send to W.

Affirmation: T is linear.

Let \(x,y \in \mathbb{R}^{2}\) and \(\alpha \in \mathbb{R}\)

Then, \(T(\alpha \cdot x +y) = (2(\alpha x_1 + y_1 + ) + (\alpha x_2 + y_1), \alpha x_1 + y_1)\) - Notice that we keep the structure of the codomain once applied to sent to W.

The following is only algebra:

\(=(2\alpha x_1 + 2y_1 + \alpha x_2 + y_2,\;\alpha x_1 + y_1)\)

Notice that we may reorder the values while keeping the addition:

\(=(2\alpha x_1 + \alpha x_2, \alpha x_1)\;+\;(2y_1 + y_2, y_1)\)

\(=\alpha (2x_1 + x_2, x_1) + (2y_1 + y_2, y_1\) - If you become confused with these steps, you can start noticing that this is what you want to obtain.  

\(=\alpha \cdot T(x) + T(y)\)

\end{document}